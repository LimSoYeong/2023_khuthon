,Title,thumbnail,text
0,Hadoop Yarn 아키텍쳐,https://velog.velcdn.com/images/matteblack9/post/484a0e4a-4e7e-4ee0-a5fc-1fefb1283834/image.png,"개발Hadoop을 배우게 되면, MapReduce 다음으로 Yarn의 개념을 가장 많이 접하게 되는데, 대부분은 단순히 리소스 매니저라는 단어 정도로만 머릿속에 넣고, 지나간다. 하지만, 문제 해결을 하는 데 있어서, 문제의 본질과 핵심에 대해 깊이 알지 못하면 결국 그것을 해결하지 못한다는 믿음에 근거하여, 정리를 남기고자 한다.먼저 Yarn이 Hadoop 2.0 부터 나오게 된 배경부터 살펴보자. Hadoop 1.0Hadoop 1.0에서는 Data Processing 부분과 Resource Management 부분을 하나의 Layer로 관리해왔음을 알 수 있는데, 이를 좀 더 자세히 말하자면Job Tracker와 Task Tracker라는 두 개의 구성 요소를 생각하면 되겠다. Job Tracker:
클러스터에서 현재 얼마의 resource가 가용한지 등을 모니터링하는 리소스 관리 기능 을 수행하고, MapReduce 잡을 스케줄링 하고 배포하고 모니터링하는 실행 관리 기능 을 수행한다. Task Tracker:
Job Tracker에게 수행될 Task의 ID를 받아와, 그 Task의 Processing 현황을 Job Tracker에게 보고하는 제한적인 기능만을 수행한다.또한 Job Tracker는 Name Node(NN) 상에서 수행이 되는 데, 클러스터에서 단 하나의 name node가 hadoop 1.0의 구조상 모든 클러스터의 노드를 관리하는 구조로 되어있어, single point of failure 의 문제점이 있다. 클러스터 관리자가 자체적으로 stand by node를 활성화하도록 해야한다. 그래서 Hadoop 1.0의 경우에는 Job Tracker 가 약 4000개의 노드, 40000개의 태스크까지만 관리할 수 있다고, 알려져있다. Hadoop 2.0하둡 2.0부터는 1.0의 많은 부족한 부분들이 보완되었다. 크게 3가지로 보면되는데, Job Tracker가 담당했던 클러스터 리소스 관리, 실행 관리 부분 등을 여러개의 컴포넌트로 나누어, 하나의 클러스터에 더 많은 노드와 태스크를 관리할 수 있도록 하였다. MapReduce 모델 뿐 아니라, Spark, HBase, Storm등의 데이터 처리 모델을 지원하게 되었다. Application Master에 특정한 어플리케이션 처리 라이브러리를 올림으로써, 하나의 하둡 클러스터에서 다양한 어플리케이션이 돌아가는 것이 가능해졌다.Zookeeper를 이용한 리소스 매니저의 이중화로 SPOF문제를 해결함.Hadoop 2.0을 이해하기 위해, 그림에서 나온 컴포넌트들을 이해해야 하는데, 먼제 다음 그림을 보자.Resource Manager:
리소스 할당과 관리, 스케줄링을 책임진다. Node Manager 로부터 노드의 상태와 노드의 리소스 가용량을 전달 받고, Application Manager에게는 job을 실행하는데 필요한 리소스를 요청받고, 이에 응하여 할당해준다. Resource Manager는 크게 Scheduler와 Application Master 두가지로 나눠진다.Scheduler : 가용한 리소스와 할당해야할 어플리케이션의 상태를 판단해 오직 스케줄링만 하는 역할을 한다.Application Manager : Application Master와 헷갈리기 쉬운 개념인데, job 제출 후, 어플리케이션이 실행될 때 어플리케이션의 Application Master를 할당하고 실행시키는 역할이라고 보면 되겠다. Application Master가 실패하면, 다시 재시작도 해준다. Node Manager:
노드 마다 하나씩 있으며, 해당 노드에 돌아가는 Application Master의 요청에 따라 컨테이너의 생성, 삭제와 실행을 책임진다. 또한, Resource Manager 에게 지속적으로 Health Status를 보내며, 리소스 사용량 등을 보고하여, 클러스터의 현재 노드 상태를 알 수 있도록 한다.Application Master:
어플리케이션 마스터는 말 그대로 제출된 job을 책임지는 역할을 한다. Resource Manager와 Job 실행에 필요한 자원을 지속적으로 요청하면서, 어플리케이션의 실행 상태에 대해 트래킹하고, 진행사항을 모니터링 한다. Application Master의 정확한 정의는 특정 프레임워크 별로 Job을 실행시키기 위한 별도의 라이브러리이다.
MapReduce는 MapReduce Application Master
Storm은 Storm Application Master
Spark는 Spark Application Master
에 올리는 것이라 보면된다. 하둡 2.0은 크게 3가지 인터페이스로 통신한다.Client<-->ResourceManagerApplicationClientProtocolApplicationMaster<-->ResourceManagerApplicationMasterProtocolApplicationMaster<-->NodeManagerContainerManagementProtocol사용자는 Job을 ResourceManager에게
ApplicationClientProtocol을 통해 전달한다.job을 제출한 후, Application Master는 ApplicationClientProtocol을 통해, job이 실행되는 데 필요한 리소스를 RM에게 요청하고, 태스크가 실행하는 동안, Application Master는 Node Manager와 ContainerManagementProtocol 을 통해 통신을 하며, 모든 컨테이너와 관련한 이벤트들은 이 프로토콜을 통해 이루어진다고 보면된다.
노드 매니저는 Container Launch Context(CLC)를 Application Master에게 받아, 태스크를 실행시킨다. 출처:
https://hadoop.apache.org/
https://blog.skcc.com/
https://www.geeksforgeeks.org
https://www.tutorialspoint.com[Spark Issue] Fetch Failed Exception 문제 분석하기Spark 아키텍쳐와 실행"
1,하둡 프로그래밍 - MapReduce,https://velog.velcdn.com/images%2Fspdlqjfire%2Fpost%2F9188daef-72f9-4658-878f-7ccafefd7ca2%2Fimage.png,"하둡하둡은 기본적으로 HDFS, MapReduce로 구성된다. 맵리듀스는 HDFS에 저장된 파일을 분산 배치 분석을 할 수 있게 도와주는 프레임 워크이다. 개발자는 맵리듀스 프로그래밍 모델에 맞게 애플리케이션을 구현한다. 데이터 전송, 분산 처리, 내고장성 등의 복잡한 처리는 맵리듀스 프레임워크가 자동으로 담당하는 Runtime System이다.1. MapReduce의 개념맵리듀스 프로그래밍 모델은 Map과 Reduce라는 크게 2가지 단계로 데이터를 처리한다. 맵은 입력 파일을 한 줄씩 읽어 데이터를 변형(Transformation)하며 리듀스는 맵의 결과 데이터를 집계(Aggregation)한다. 다만, 개발자는 맵의 데이터 Transformation 규칙을 자유롭게 구현하여 정의할 수 있다.
이러한 맵리듀스 프로그래밍 모델은 다음과 같이 함수로 표현할 수 있다.맵 : (key1, value1) => list(key2, value2) 리듀스 : (key2, list(value2)) => (key3, list(value3))2. MapReduce Architecture맵리듀스 프레임워크는 Runtime System이다. 개발자는 단순하게 분석 로직을 구현하고, 데이터에 대한 분산 및 병렬 처리에 대한 어려운 문제는 프레임워크가 전담한다. 그렇기에 개발자가 맵리듀스 아키텍처를 몰라도 개발은 가능하다. 다만, 아키텍처를 제대로 알지 못한채 개발하게 되면 성능을 고려하지 않게 되므로, 이는 바람직하지 않다.맵리듀스 시스템은 JobTracker, TaskTracker로 구성되며 이 또한 HDFS와 마찬가지로 master-slaves 구조이다. 클라이언트는 JobTracker에게 Job 실행을 요청하고, Job의 진행 상황 및 완료 결과를 공유받게 된다.
클라이언트란, 사용자가 실행한 맵리듀스 프로그램과 하둡에서 제공하는 맵리듀스 API를 말한다. 
클라이언트란, 사용자가 실행한 맵리듀스 프로그램과 하둡에서 제공하는 맵리듀스 API를 말한다. 
클라이언트가 실행을 요청하는 맵리듀스 프로그램은 Job 이라고 하는 하나의 단위로 관리된다. 이러한 Job은 여러 개의 task들로 분산되어 처리된다. Job Tracker는 하둡 클러스터에 있는 전체 Job의 스케줄링을 관리하며 모니터링한다. 전체 하둡 클러스터에서는 하나의 JobTracker가 구동되게 된다. 보통 Job Tracker는 네임 노드가 구동되는 서버에서 동작한다.
사용자가 맵리듀스 Job을 요청하게 되면 JobTracker는 몇 개의 맵과 리듀스를 실행하게 될 지를 계산한다. 이러한 task들을 어떤 TaskTracker에서 실행할 것인지를 결정하며, Task를 할당한다. JobTracker와 TaskTracker는 Heartbeats message를 사용하여 TaskTracker의 상태와 작업 실행 정보를 공유한다.
클라이언트가 실행을 요청하는 맵리듀스 프로그램은 Job 이라고 하는 하나의 단위로 관리된다. 이러한 Job은 여러 개의 task들로 분산되어 처리된다. Job Tracker는 하둡 클러스터에 있는 전체 Job의 스케줄링을 관리하며 모니터링한다. 전체 하둡 클러스터에서는 하나의 JobTracker가 구동되게 된다. 보통 Job Tracker는 네임 노드가 구동되는 서버에서 동작한다.
사용자가 맵리듀스 Job을 요청하게 되면 JobTracker는 몇 개의 맵과 리듀스를 실행하게 될 지를 계산한다. 이러한 task들을 어떤 TaskTracker에서 실행할 것인지를 결정하며, Task를 할당한다. JobTracker와 TaskTracker는 Heartbeats message를 사용하여 TaskTracker의 상태와 작업 실행 정보를 공유한다.
Task Tracker는 사용자가 설정한 맵리듀스 프로그램을 실행하며, 하둡의 데이터 노드에서 실행되는 데몬이다. JobTracker로부터 작업을 요청받으며, 그러면 map task와 reduce task를 생성하게 된다. 이러한 task가 생성되면 새로운 JVM을 구동해 task를 실행한다. 이 때 task를 실행하기 위한 JVM은 재사용될 수도 있다. 또한, 하나의 데이터 노드이더라도 여러 개의 JVM을 구동하여 데이터를 동시에 분산 처리하게 된다.
Task Tracker는 사용자가 설정한 맵리듀스 프로그램을 실행하며, 하둡의 데이터 노드에서 실행되는 데몬이다. JobTracker로부터 작업을 요청받으며, 그러면 map task와 reduce task를 생성하게 된다. 이러한 task가 생성되면 새로운 JVM을 구동해 task를 실행한다. 이 때 task를 실행하기 위한 JVM은 재사용될 수도 있다. 또한, 하나의 데이터 노드이더라도 여러 개의 JVM을 구동하여 데이터를 동시에 분산 처리하게 된다.3. MapReduce Work Flow위 그림을 다시 보도록 하자. 우선 입력 데이터가 들어오게 되면 데이터를 Input Split 단위로 쪼개게 된다. 이 때 Input Split의 크기는 일반적으로 HDFS의 블록 크기와 동일하다. 그 이유는 추후 설명한다. 이 과정을 Splitting 이라고 한다.그 다음 Mapping 과정을 거친다. 이 때 Input Split의 데이터를 레코드 단위로 한 줄씩 읽어서 사용자가 정의한 Map Function을 적용한다. Input Split은 위 그림에서 1줄로 표현되어 있으나, 실제로는 64MB(하둡 2.0부터는 128MB)이므로 상당히 많은 레코드가 있을 것이다. 즉, Input Split의 레코드 수 만큼 Map Function이 적용되게 되고 하나의 Input Split에 하나의 Mapper Class가 적용된다. Map Task의 결과로 intermediate Key-Value Pair가 생성된다. 위 그림에서 보듯이 Deer, 1과 같은 Pair가 생성됐음을 알 수 있다. 그러면 Reduce Task는 이 데이터를 내려 받아 Shuffling을 한다. Shuffle 과정은 Map Task의 데이터가 Reduce Task로 전달되는 과정이다. 여기서 네트워크 통신이 가장 많이 일어나게 되며 그에 따라 MapReduce 성능의 저하가 가장 많이 발생하는 구간이다. Shuffle 과정에서는 Partition 과정도 존재한다. Partitioner는 맵의 출력 레코드를 읽어서 출력 키의 해시값을 구한다. 각 해시값은 레코드가 속하는 Partition 번호로 사용되게 된다. 이 때 Partition은 Reduce Task의 개수만큼 생성된다. Partitioner는 개발자가 재정의하여 입맛에 맞게 사용할 수도 있다.또한, 위 그림에서는 최종 결과가 1개 이므로 Reducer가 1개가 있다는 의미이다. Reducer의 개수에 따라 출력 파일이 그 개수에 맞게 생성되기 때문이다.4. MapReduce Programming Elements맵리듀스는 네트워크 통신을 위한 최적화된 객체로 WritableComparable Interface를 제공한다. 맵리듀스 프로그램에서 사용하는 모든 Key와 Value는 반드시 WritableComparable Interface가 구현되어 있어야 한다. 하둡에서 제공하는 데이터 타입은 기본적으로 모두 WritableComparable Interface가 구현되어 있으며, 개발자가 원하는 경우 직접 해당 인터페이스를 사용하여 구현할 수도 있다.WritableComparable Interface는 Writable Interface와 Comparable Interface를 상속한 인터페이스이다. Comparable Interface의 경우 java.lang 패키지의 인터페이스로 정렬을 처리하기 위해 compareTo 메소드를 제공한다. Writable Interface의 write 메소드는 데이터 값을 serialization(직렬화)하고, readFields 메소드는 deserialization을 진행한다. 데이터를 전송할 때는 구조화된 객체를 직접적으로 전송할 수 없다. 따라서 이를 바이트 스트림으로 바꿔줘야 하는데 그 과정이 serialization이다. 이것은 구조화된 객체를 네트워크 전송을 위해 바이트 스트림으로 변환하거나, 하드 디스크에 저장하기 위한 것이다. 반대로 deserialization은 네트워크를 통해 전송받은 바이트 스트림을 구조화된 객체로 변환하는 것이다. 맵리듀스 API는 자주 사용하는 데이터 타입에 대한 WritableComparable Interface를 구현한 Wrapper Class를 제공한다. 이 때 제공되는 데이터 타입은 다음과 같다.맵리듀스는 Input Split을 맵 메소드의 입력 파라미터로 사용할 수 있도록 InputFormat이라는 추상 클래스를 제공한다. InputFormat Class는 Input Split을 맵 메소드가 사용할 수 있돌고 getSplits 메소드를 제공한다. 그리고 createRecordReader 메소드는 Input Split을 Key와 List 형태로 사용할 수 있게 RecordReader 객체를 생성한다. 아래 표는 InputFormat 유형 표이다.Mapper Class의 소스 코드를 보면 제너릭 파라미터를 이용해 클래스를 정의한다. 위 코드에서 <> 내부 내용은 <입력 키 유형, 입력 값 유형, 출력 키 유형, 출력 값 유형>을 말한다. 그리고 Mapper Class 에서는 Context 객체를 선언하며, 이를 사용해 Job에 대한 정보를 얻어올 수 있게 된다.위 코드는 map 함수이다. MapReduce 프로그램을 개발할 때 위 함수를 재정의하여 사용한다.Reducer Class의 소스 코드를 보면 Mapper Class와 마찬가지로 제너릭 파라미터를 사용한다. 위 코드에서 <> 내부 내용은 Mapper와 동일하다.위 코드는 reduce 함수이다. map 함수와 마찬가지로 MapReduce 프로그램을 개발할 때 위 함수를 재정의하여 사용한다.이전에 설명했듯 MapReduce 과정에서 Shuffling 과정은 가장 오버헤드가 크게 발생하는 구간이다. 네트워크 통신이 가장 많이 발생하기 때문이다. 이를 줄이고자 Combiner Class가 등장하게 됐다. Combiner Class는 Shuffle할 데이터의 크기를 줄이는 것에 도움을 주게 된다. Combiner Class는 Mapper의 출력 데이터를 입력 데이터로 받아 그 크기를 줄이는 것에 목적을 두고 있다. 따라서, 네트워크 비용은 발생하지 않는다. Combiner Class를 적용한 이후에 Shuffle 과정에서 통신할 데이터의 양이 줄었을 것이고, 그럼으로써 비용 감소 및 성능 향상을 이끌어낼 수 있을 것이다.다만, 주의할 점이 있다. Combiner Class를 적용하지 않아도 MapReduce 알고리즘이 정상적으로 동작하게끔 설계해야 한다는 것이다. 즉, 적용했든 안했든 모두 같은 결과를 출력해야 한다는 것이다.그렇다면, MapReduce Job의 OutputFormat에 대해서도 알아보도록 하자. 아래 표는 OutputFormat에 관한 표이다.5. MapReduce Job Execution맵리듀스 Job이 실행되는 과정은 다음과 같다.클라이언트는 org.apache.hadoop.mapreduce.Job의 waitForCompletion 메소드를 호출해 Job 실행을 요청한다. 이 때, 클라이언트의 요청은 Job의 내부 컴포넌트인 JobClient에게 전달된다. JobClient는 JobTracker의 getNewJobID 메소드를 호출해 새로운 Job ID를 요청한다. JobTracker는 Job의 출력 파일 경로가 정상적인지 확인하고 Job ID를 발급한다. 만약 Job의 출력 파일 경로가 비정상적일 경우 에러를 리턴한다. 하둡은 기본적으로 RPC로 통신하며, JobSubmissionProtocol에 정의된 Protocol을 사용한다.JobClient는 Job을 실행하는 데 필요한 정보를 JobTracker와 TaskTracker에게 공유해야 한다. 그래서 JobClient는 Input Split의 정보, JobConf에 설정된 정보, Job Class 파일 혹은 Job Class가 포함된 JAR 파일을 HDFS에 저장한다. 그러면 하둡 클러스터의 모든 노드에서 위 정보와 파일에 접근할 수 있기 때문이다.JobClient는 Job Tracker의 submitJob 메소드를 호출해 Job 실행을 요청한다.이제 Job 실행을 요청하게 됐다. JobTracker는 Job을 실행하기 위한 초기 설정 작업을 진행하게 된다. JobTracker는 Job의 상태와 진행 과정을 모니터링할 수 있는 JobInProgress를 생성한다. 이 때 JobInProgress는 JobClient가 HDFS에 등록한 Job 공통 파일을 로컬 디스크로 복사한 후 Split 정보를 이용해 Map Task 개수와 Reduce Task 개수를 계산한다. 또, Job의 실행 상태를 RUNNING으로 설정한다.JobTracker는 생성한 JobInProgress 객체를 내부 큐인 jobs에 등록한다. 큐에 등록된 JobInProgress는 스케쥴러에 의해 소비되게 된다.맵리듀스는 Task를 할당하기 위한 스케줄러인 TaskScheduler을 제공한다. TaskScheduler는 추상 클래스이며, 이를 구현한 것은 3가지 이다.FIFO(First In First Out) 방식의 JobQueueTaskSchedulerFair SchedulerCapacityScheduler이 때 참고로 , TaskScheduler는 JobTracker 내부에서 운용된다.TaskTracker는 3초에 1번씩 JobTracker에게 Heartbeats message를 전송한다. 이를 통해 TaskTracker가 실행 중이라는 것과 새로운 Task를 실행할 준비가 됐다는 것을 알려 준다.Scheduler는 TaskTracker의 Heartbeats message를 확인한 후, 내부 큐(JobInProgress가 들어 있는 jobs)에서 할당할 job을 선택한다. 그리고 해당 job에서 하나의 task를 선택한다. 참고로, 하나의 job은 다수의 task로 구성된다. 이 때 job을 선택하는 기준은 scheduler 마다 그 기준이 다르다.Scheduler는 Map Task와 Reduce Task를 구분해 Task를 할당한다. 이 때 Task는 Data Locality를 고려하여 할당하게 된다. 하둡의 맵리듀스는 Computing to Data 패러다임을 채택했고, 사용하기 때문이다.Scheduler는 Task를 선택한 후 해당 TaskTracker에게 Task 할당을 알려 준다. JobTracker는 TaskTracker가 전송한 Heartbeats의 응답으로 HeartbeatResponse를 전송한다. JobTracker는 TaskTracker에게 지시할 내용을 HeartbeatResponse에 담아 설정하게 된다. 그래서 Scheduler는 HeartbeatResponse에 Task 실행을 요청한다. 해당 메시지에는 task 실행 및 종료, job 종료, tasktracker 초기화 재실행, task 완료 등의 작업을 설정할 수 있다.TaskTracker는 할당받은 task를 새로운 JVM에서 실행하게 된다. 맵리듀스는 이를 Child JVM 이라고 표현한다. 이 때 새로운 JVM에서 발생하는 버그는 TaskTracker에게 영향을 끼치지 않기 때문에 안정적으로 TaskTracker를 운영할 수 있다. 또한, Child JVM은 재사용이 가능하다. TaskTracker 내부의 TaskLauncher는 HeartbeatResponse에서 Task 정보를 꺼내서 Task의 상태와 진행 과정을 모니터링할 수 있는 TIP(TaskInProgress)를 생성한다. TaskTracker는 HDFS에 저장된 공통 파일을 로컬 디렉토리로 복사한다. 그 다음 TaskInProgress는 Task 실행 결과를 저장할 로컬 디렉토리를 생성한 후 Job JAR 파일을 해당 디렉토리에 풀어 놓는다.TaskInProgress는 TaskRunner에게 Task 실행을 요청한다.TaskRunner는 JvmManager에게 Child JVM에서 task를 실행해 줄 것을 요청한다.JvmManager는 실행할 클래스 명과 옵션을 설정한 후, 커맨드 라인에서 Child JVM을 실행한다. 이 때, Child JVM은 TaskUmbilicalProtocol 인터페이스로 부모 클래스와 통신하게 된다. Child JVM은 Task가 완료될 때까지 Task의 진행 과정을 주기적으로 JvmManager에게 알려 준다. 그러면 TaskTracker는 해당 정보를 공유받아 Task의 진행 과정을 모니터링할 수 있다.최종적으로 사용자가 정의한 Mapper Class 혹은 Reducer Class가 실행된다.TaskTracker가 JobTracker에 전송하는 Heartbeats에는 완료된 Task의 정보가 포함된다.JobTracker는 해당 Job이 실행한 전체 Task의 완료 정보를 받게 될 경우 JobInProgress는 Job의 상태를 SUCCEEDED로 변경한다. 만약 장애가 발생하여 Job이 실패했다면 Job의 상태를 FAILED로 변경한다.Job을 실행한 클라이언트와 JobClient는 Job이 완료될 때 까지 대기하고 있으며 JobClient는 JobTracker의 getJobStatus 메소드를 호출해 job의 상태를 확인한다. JobClient는 job의 상태가 SUCCEED이면 true를, FAILED면 false를 클라이언트에게 전달한다.클라이언트는 최종 결과를 출력하고, job 실행을 완료한다.하둡 프로그래밍 - HDFS하둡 프로그래밍 - 미국 항공편 운항 통계 데이터 분석"
2,OpenSearch에 대해 알아보자!,https://velog.velcdn.com/images/sosimina/post/b0ce3a7d-98c3-40e6-b6cf-f3be39b69552/image.png,"Opensearch는 Elastic Search가 기업화를 꾀하는데에 대한 반발로 나온 오픈소스이다!
https://aws.amazon.com/ko/blogs/opensource/introducing-opensearch/
https://aws.amazon.com/ko/what-is/opensearch/
https://opensearch.org/docsElastic Search 7.1에서 Fork 되어서 등장Kibana 7.1에서 Fork 되어서 등장ALv2 라이센스 (무료 개방형 오픈소스)분산형 커뮤니티 기반 100% 오픈 소스 검색 및 분석 제품군으로, 실시간 애플리케이션 모니터링, 로그 분석 및 웹 사이트 검색과 같이 다양한 사용 사례에 사용opensearch는 기존 ES (Elastic Search)를 대체해서 등장했기에 다양한 스택과 조합이 가능.수집 및 정제Data prepper (https://opensearch.org/docs/1.2/clients/data-prepper/index/) : Server side collector / logstash를 fork해서 나왔고 56프로 더 빠르다고 함.BeatsFluentd (https://docs.fluentd.org/output/opensearch)logstash 대시보드그라파나오픈서치 대시보드(Kibana를 Fork해서 괜찮은 대시보드를 보여줌)https://opensearch.org/docs/latest/opensearch/cluster/Opensearch는 ES와 같이 Master & Data node로 클러스터에서 나뉘어져 있다.Opensearch를 시작할때 필수로 해야하는 부분이 클러스터 생성이다.
Operator를 활용해 CRD로 리소스를 새롭게 정의해 사용한다.Cluster 	구성각각 노드마다의 attribute 부여 / Coordinating , Cluster Manager, Data nodeCluster에 Specific IP 부여Host 탐색 설정Cluster StartShard 구성 (Advanced)hot&warm architecture https://opensearch.org/docs/latest/security-plugin/audit-logs/index/Audit log는 cluster에 대한 access를 추적가능Compliance & Security 측면에서 유리하다Add the following line to opensearch.yml on each node:Storage Type관련
https://opensearch.org/docs/latest/security-plugin/audit-logs/storage-types/
https://opensearch.org/docs/latest/security-plugin/audit-logs/storage-types/Restart the node검색 속도 높이기ShardingIndex data를 여러 노드에 분산 저장하여 매핑하고 이러한 샤드를 클러스터 전체에 분산저장한다면 검색속도를 높일 수 있다.Opensearch는 ElasticSearch가 7.10 이후로 라이센스를 변경하며 탄생하여 대안이 될 수 있다.AWS 및 다양한 기업에서 후원중.ELK/EFK vs PLGFluentd vs FluentBit"
3,{일상} 10분만 일상,https://velog.velcdn.com/images/pairkorean/profile/e46b9e1c-a98a-4a37-9041-50afedf979a0/social_profile.png,"{일상}10분동안 할게 뭐 있다고, 아둥바둥 힘들게 기록하나 싶다가도나한테 올바른 것이라면, 10분이든 어디서든, 나까지 손해보는 짓이라고 착각하지 말기나쁜짓만 아니면 아무것도 안하는 것보다 왠만하면 낫다 기억하기주말만 되면 너무 큰 목표를 세우는데, 실은 숨가쁜 주중이랑 비슷비슷오전에는 발표연습, 가능하면 플러터 플로우 맛보기정도일요일에는 4시간만 해보자. 오전 2시간 오후 2시간{일상} 빙글빙글{일상} 갤럭시 생태계 구축"
4,Airflow,https://images.velog.io/images/ifelifelse/post/5c0a676a-26f7-43e2-9025-6a88749d0876/image.png,"강의정리 - MLOps목차
Apache Airflow 소개
-- Batch Process란?
-- Batch Process - Airflow 등장 전
-- Airflow 소개 
Apache Airflow 소개
-- Batch Process란?
-- Batch Process - Airflow 등장 전
-- Airflow 소개 
Apache Airflow 실습하며 배워보기
-- 설치하고 실행하기
-- DAG 작성하기
-- 유용한 Operator 간단 소개
Apache Airflow 실습하며 배워보기
-- 설치하고 실행하기
-- DAG 작성하기
-- 유용한 Operator 간단 소개
Apache Airflow 아키텍처와 활용방안
-- 기본 아키텍처
-- Airflow 실제 활용 사례
-- MLOps 관점의 Airflow
Apache Airflow 아키텍처와 활용방안
-- 기본 아키텍처
-- Airflow 실제 활용 사례
-- MLOps 관점의 AirflowApache Airflow 소개Batch Process란?예약된 시간에 실행되는 프로세스일회성(1회)도 가능하고, 주기적인 실행도 가능
-- 이번 주 일요일 07:00에 1번 실행되는 프로세스
-- 매주 일요일 07:00에 실행되는 프로세스Batch Process를 AI엔지니어가 알아야 하는 이유모델을 주기적으로 학습시키는 경우 사용(Continuous Training)주기적인 Batch Serving을 하는 경우 사용그 외 개발에서 필요한 배치성 작업Batch Process - Airflow 등장 전대표적인 Batch Process 구축 방법: Linux Crontab서버에서 crontab -e 입력실행된 에디터에서 0****predict.py 입력(0****은 크론탭 표현으로 매 시 0분에 실행하는 것을 의미) OS에 의해 매 시 0분에 predict.py가 실행Linux는 일반적인 서버 환경이고, Crontab은 기본적으로 설치되어 있기 때문에 매우 간편간단하게 Batch Process를 시작하기에 Crontab은 좋은 선택 크론 표현식
Batch Process의 스케줄링을 정의한 표현식
Batch Process의 스케줄링을 정의한 표현식
이 표현식은 다른 Batch Process도구에서도 자주 사용 됨

크론 표현식 예시

이 표현식은 다른 Batch Process도구에서도 자주 사용 됨

크론 표현식 예시

매번 표현식을 암기할 필요는 없지만, 읽을 정도만 인지하면 좋음
매번 표현식을 암기할 필요는 없지만, 읽을 정도만 인지하면 좋음
크론 표현식 제너레이터 사이트가 많이 있으니, 활용
-- 크론 표현식 제너레이터 사이트 (시간->크론표현식) 
크론 표현식 제너레이터 사이트가 많이 있으니, 활용
-- 크론 표현식 제너레이터 사이트 (시간->크론표현식) 
cron 표현식이 어렵다면 다음 사이트에서 확인 가능
-- 크론탭 구루(크론표현식->시간)
cron 표현식이 어렵다면 다음 사이트에서 확인 가능
-- 크론탭 구루(크론표현식->시간)Linux Crontab의 문제재실행 및 알람
-- 파일을 실행하다 오류가 발생한 경우, 크론탭이 별도의 처리를 하지 않음
-- 예) 매주 일요일 07:00에 predict.py를 실행하다가 에러가 발생한 경우, 알람을 별도로 받지 못함실패할 경우, 자동으로 몇 번 더 재실행(Retry)하고, 그래도 실패하면 실패했다는 알람을 받으면 좋음과거 실행 이력 및 실행 로그를 보기 어려움여러 파일을 실행하거나, 복잡한 파이프라인을 만들기 어려움-> Crontab은 간단히 사용할 수는 있지만, 실패 시 재실행, 실행 로그 확인, 알람 등의 기능은 제공하지 않음--> 좀 더 정교한 스케줄링 및 워크플로우 도구가 필요함다양한 도구
스케줄링 워크플로우 전용 도구의 등장
Airflow 소개현재 스케줄링, 워크플로우 도구의 표준에어비앤비(Airbnb)에서 개발현재 릴리즈된 버전은 2.2.0으로, 업데이트 주기가 빠름스케줄링 도구로 무거울 수 있지만, 거의 모든 기능을 제공하고, 확장성이 넓어 일반적으로 스케줄링과 파이프라인 작성 도구로 많이 사용특히 데이터 엔지니어링 팀에서 많이 사용Airflow가 제공하는 기능파이썬을 사용해 스케줄링 및 파이프라인 작성 ! 스케줄링 및 파이프라인 목록을 볼 수 있는 웹 UI제공 !실패 시 알람실패 시 재실행 시도동시 실행 워커 수설정 및 변수값 분리 Apache Airflow 실습하며 배워보기설치하고 실행하기Airflow에서 사용할 어드민 계정 생성Airflow Webserver 실행http://localhost:8080으로 접속하면 웹 UI 등장. 위에서 생성한 어드민 계정으로 로그인 해보자.
웹UI대시보드 화면이 보인다. 하지만 스케줄러 실행중이지 않다는 경고가 보임-> 별도의 터미널 창을 띄워 다음처럼 Airflow Scheduler를 실행한다.-> 다른 경고로 바뀜 :)
->없어지는거 맞음 나의 실수였다. ㅎㅎ..정리Airflow 설치
-- pip install apache-airflowAirflow기본 디렉토리 설정
-- 환경변수 AIRFLOW_HOME에 사용할 기본 디렉토리 경로 설정
-- export AIRFLOW_HOME=/abc/def/Airflow DB 초기화
-- Airflow에서 사용할 DB를 초기화
-- airflow db initAirflow 어드민 계정 생성
-- airflow user create Airflow 웹서버 실행
-- airflow webserverAirflow 스케줄러 실행
-- airflow scheduler DAG 작성하기Batch Scheduling을 위한 DAG 생성Airflow에서는 스케줄링할 작업을 DAG이라고 부름DAG은 Directed Acyclic Graph의 약자로, Airflow에 한정된 개념이 아닌 소프트웨어 자료구조에서 일반적으로 다루는 개념 DAG은 이름 그대로, 순환하지 않는 방향이 존재하는 그래프를 의미Airflow는 Crontab처럼 단순히 하나의 파일을 실행하는 것이 아닌, 여러 작업의 조합도 가능함DAG 1개 : 1개의 파이프라인Task : DAG내에서 실행할 작업하나의 DAG에 여러 Task의 조합으로 구성된다.
예) tutorial_etl_dag 이라는 DAG은 3가지 Task로 구성
extracttransformload
tutorial_etl_dag이라는 DAG을 실행하면 이 3가지 Task를 순차적으로 실행Task가 꼭 순차적으로 진행하지 않게 할 수도 있음
tutorial DAG
print_data Task 이후 sleep, templated Task 동시 실행Airflow는 DAG이라는 단위로 스케줄링 관리각 DAG은 Task로 구성DAG 내 Task는 순차적으로 실행되거나, 동시에 (병렬로)실행할 수 있음DAG작성하기 - hello_world먼저 DAG을 담을 디렉토리 생성(이름은 무조건 dags)
AIRFLOW_HOME에 dags 생성했다.
AIRFLOW_HOME/dags/{}.py 파일 생성 이제 파일을 저장하고, 웹 UI를 확인해보면 새로 생성한 DAG이 보임 조금 기다리면 실행된 결과를 볼 수 있음

세로 한 줄이 하나의 실행을 의미
-- 맨위의 원이 하나의 DAG 실행을 의미하며, 하나의 실행을 DAG Run이라고 부름
-- DAG 스케줄링 시작 날짜를 2일전으로 설정해서, 두개의 DAG Run이 생성됨
-- 내일 오전 6시(UTC 기준)가 지나면, 하나의 DAG Run 이 또 생길 것
세로 한 줄이 하나의 실행을 의미
-- 맨위의 원이 하나의 DAG 실행을 의미하며, 하나의 실행을 DAG Run이라고 부름
-- DAG 스케줄링 시작 날짜를 2일전으로 설정해서, 두개의 DAG Run이 생성됨
-- 내일 오전 6시(UTC 기준)가 지나면, 하나의 DAG Run 이 또 생길 것
아래 사각형은 하나의 Task를 의미
-- 2개의 Task를 정의했으므로, 2개의 사각형을 볼 수 있음
아래 사각형은 하나의 Task를 의미
-- 2개의 Task를 정의했으므로, 2개의 사각형을 볼 수 있음마우스를 올리면 간단한 정보를 볼 수 있음
사각형(Task)를 눌러서 Log를 눌러보면 의도한대로 echo Hello, print('World') 실행된 것을 확인할 수 있음

만약 특정 DAG Run의 기록을 지우고 다시 실행시키고 싶으면 동그라미(DAG)을 누르고 Clear를 실행DAG작성하기 - 정리AIRFLOW_HOME으로 지정된 디렉토리에 dags 디렉토리를 생성하고 이 안에 DAG파일을 작성DAG은 파이썬 파일로 작성. 보통 하나의 .py파일에 하나의 DAG을 저장DAG 파일은 크게 다음으로 구성
-- DAG 정의 부분
-- Task 정의 부분
-- Task 간 순서 정의 부분DAG 파일을 저장하면, Airflow 웹 UI에서 확인할 수 있음Airflow 웹 UI에서 해당 DAG을 ON으로 변경하면 DAG이 스케줄링되어 실행DAG 세부 페이지에서 실행된 DAG Run의 결과를 볼 수 있음유용한 Operator 간단 소개airflow.operators! PythonOperator
파이썬 함수를 실행
-- 함수뿐 아니라, Callable한 객체를 파라미터로 넘겨 실행할 수 있음
파이썬 함수를 실행
-- 함수뿐 아니라, Callable한 객체를 파라미터로 넘겨 실행할 수 있음
실행할 파이썬 로직을 함수로 생성한 후, Python Operator로 실행
실행할 파이썬 로직을 함수로 생성한 후, Python Operator로 실행BashOperatorBash 커맨드를 실행실행해야 할 프로세스가 파이썬이 아닌 경우에도 BashOperator로 실행 가능
-- ex. shell 스크립트, scala파일 등DummyOperator아무것도 실행하지 않음DAG내에서 Task를 구성할 때, 여러개의 Task의 SUCCESS를 기다려야 하는 복잡한 Task 구성에서 사용SimpleHttpOperator특정 호스트로 HTTP 요청을 보내고 Response를 반환파이썬 함수에서 requests모듈을 사용한 뒤 PythonOperator로 실행시켜도 무방 다만 이런 기능이 Airflow Operaotr에 이미 존재하는 것을 알면 좋음이 외에도 BranchOperatorDockerOperatorKuberntesOperatorCustomOperator(직접 Operator 구현)등등 클라우드의 기능을 추상화한 Operator도 존재(AWS,GCP 등) - Provider Packages
클라우드의 기능을 추상화
!!Tip!!외부 Third Party와 연동해 사용하는 Operator의 경우(docker, aws, gcp 등) Airflow 설치시 다음처럼 extra package를 설치해야함 다루지 않은 내용
Airflow DAG을 더 풍부하게 작성할 수 있는 방법으로 다음 내용Variable : Airflow Console에서 변수(Variable)를 저장해 Airflow DAG에서 활용Connection & Hooks : 연결하기 위한 설정(MySQL, GCP등)Sensor : 외부 이벤트를 기다리며 특정 조건이 만족하면 실행MarkerXComs : Task끼리 결과를 주고받고 싶은 경우 사용Apache Airflow 아키텍처와 활용방안기본 아키텍처DAG Directory
DAG 파일들을 저장기본경로는 $AIRFLOW_HOME/dagsDAG_FOLDER라고도 부르며, 이 폴더 내부에서 폴더 구조를 어떻게 두어도 상관없음Scheduler에 의해 .py 파일은 모두 탐색되고 DAG이 파싱Scheduler
Scheduler는 각종 메타 정보의 기록을 담당DAG Directory 내 .py 파일에서 DAG을 파싱하여 DB에 저장DAG들의 스케줄링 관리 및 담당실행 진행 상황과 결과를 DB에 저장Executor를 통해 실제로 스케줄링된 DAG을 실행Airflow에서 가장 중요한 컴포넌트Executor
Executor는 스케줄링된 DAG을 실행하는 객체로, 크게 2종류로 나뉨
-- Local Executor
-- Remote Executor
Local Executor
Executor는 스케줄링된 DAG을 실행하는 객체로, 크게 2종류로 나뉨
-- Local Executor
-- Remote Executor
Local Executor
Local Executor는 DAG Run을 프로세스 단위로 실행하며 다음처럼 나뉨
Local Executor는 DAG Run을 프로세스 단위로 실행하며 다음처럼 나뉨
하나의 DAG Run을 하나의 프로세스로 띄워서 실행
하나의 DAG Run을 하나의 프로세스로 띄워서 실행
최대로 생성할 프로세스 수를 정해야 함
최대로 생성할 프로세스 수를 정해야 함
Airflow를 간단하게 운영할 때 적합
Airflow를 간단하게 운영할 때 적합
Sequential Executor
-- 하나의 프로세스에서 모든 DAG Run들을 처리
-- Airflow 기본 executor로, 별도 설정이 없으면 이 Executor를 사용
-- Airflow를 잠시 운영할 때 적합
Sequential Executor
-- 하나의 프로세스에서 모든 DAG Run들을 처리
-- Airflow 기본 executor로, 별도 설정이 없으면 이 Executor를 사용
-- Airflow를 잠시 운영할 때 적합Remote Excutor
DAG Run을 외부 프로세스로 실행
Celery Executor
-- DAG Run을 Celery Worker Process로 실행
-- 보통 Redis를 중간에 두고 같이 사용
-- Local Executor를 사용하다가 Airflow운영 규모가 좀 더 커지면 Celery Executor로 전환
Celery Executor
-- DAG Run을 Celery Worker Process로 실행
-- 보통 Redis를 중간에 두고 같이 사용
-- Local Executor를 사용하다가 Airflow운영 규모가 좀 더 커지면 Celery Executor로 전환
Kubernetes Executor
-- 쿠버네티스 상에서 Airflow를 운영할 때 사용
-- DAG Run 하나가 하나의 Pod(쿠버네티스의 컨테이너 같은 개념)
-- Airflow 운영 규모가 큰 팀에서 사용
Kubernetes Executor
-- 쿠버네티스 상에서 Airflow를 운영할 때 사용
-- DAG Run 하나가 하나의 Pod(쿠버네티스의 컨테이너 같은 개념)
-- Airflow 운영 규모가 큰 팀에서 사용라인 블로그에 잘 정리되어 있음 !!
라인 블로그 Workers
DAG을 실제로 실행Scheduler에 의해 생기고 실행Executor에 따라 워커의 형태가 다름
-- Celery 혹은 Local Executor인 경우, Worker는 프로세스
-- Kubernetes Executor인 경우, Worker는 podDAG Run을 실행하는 과정에서 생긴 로그를 저장 Metadata Database
메타 정보를 저장Scheduler에 의해 Metadata가 쌓임보통 MySQL이나 Postgres를 사용파싱한 DAG 정보, DAG Run 상태와 실행 내용, Task 정보 등을 저장User와 Role(RBAC)에 대한 정보 저장Scheduler와 더불어 핵심 컴포넌트
-- 트러블 슈팅 시, 디버깅을 위해 직접 DB에 연결해 데이터를 확인하기도 함실제 운영환경에서는 GCP Cloud SQL이나, AWS Aurora DB등 외부 DB 인스턴스를 사용Webserver
WEB UI를 담당Metadata DB와 통신하며 유저에게 필요한 메타 데이터를 웹 브라우저에 보여주고 시각화보통 Airflow 사용자들은 이 웹서버를 이용하여 DAG을 ON/OFF하며, 현상황을 파악REST API도 제공하므로, 꼭 WEB UI를 통해서 통신하지 않아도 괜찮음웹서버가 당장 작동하지 않아도, Airflow에 큰 장애가 발생하지 않음 (반면 Scheduler의 작동여부는 매우 중요)Airflow 실제 활용 사례Airflow를 구축하는 방법으로 보통 3가지 방법을 사용1. Managed Airflow
Managed Airflow은 클라우드 서비스 형태로 Airflow를 사용하는 방법보통 별도의 데이터 엔지니어가 없고, 분석가로 이루어진 데이터 팀의 초기에 활용하기 좋음Managed Airflow의 장단점장점설치와 구축을 클릭 몇번으로 클라우드 서비스가 다 진행유저는 DAG 파일을 스토리지(파일 업로드)형태로 관리단점비용자유도가 적음. 클라우드에서 기능을 제공하지 않으면 불가능한 제약이 많음2. VM + Docker Compose
참고

VM + Docker compose는 직접 VM위에서 Docker compose로 Airflow를 배포하는 방법
Airflow 구축에 필요한 컴포넌트(Scheduler, Webserver, Database 등)를 Docker container 형태로 배포
예시)
쏘카 참고VM + Docker compose 방법의 장단점장점Managed Service 보다는 살짝 복잡하지만, 어려운 난이도는 아님
-- (Docker와 Docker compose에 익숙한 사람이라면 금방 익힐 수 있음)
-- 하나의 VM만을 사용하기 때문에 단순단점각 도커 컨테이너 별로 환경이 다르므로, 관리 포인트가 늘어남예를들어, 특정 컨테이너가 갑자기 죽을수도 있고, 특정 컨테이너에 라이브러리를 설치했다면, 나머지 컨테이너에도 하나씩 설치해야함.Kubernetes + Helm
Kubernetes+ Helm은 Kubernetes 환경에서 Helm 차트로 Airflow를 배포하는 방법Kubernetes는 여러개의 vm을 동적으로 운영하는 일종의 분산환경으로, 리소스 사용이 매우 유연한게 대표적인 특징(필요에 따라 VM수를 알아서 늘려주고 줄여줌)이런 특징 덕분에, 특정 시간에 배치 프로세스를 실행시키는 Airflow와 궁합이 매우 잘맞음Airflow DAG 수가 몇 백개로 늘어나도 노드 오토 스케일링으로 모든 프로세스를 잘 처리할 수 있음하지만 쿠버네티스 자체가 난이도가 있는만큼 구축과 운영이 어려움보통 데이터팀에 엔지니어링 팀이 존재하고, 쿠버네티스 환경인 경우에 적극 사용
쏘카 참고MLOps 관점의 AirflowAirflow는 데이터 엔지니어링에서 많이 사용하지만, MLOps에서도 활용할 수 있음
""주기적인 실행""이 필요한 경우Batch Training: 1주일 단위로 모델 학습Batch Serving(Batch Inference): 30분 단위로 인퍼런스인퍼런스 결과를 기반으로 일자별, 주차별 모델 퍼포먼스 Report 생성MySQL에 저장된 메타데이터를 데이터 웨어하우스로 1시간 단위로 옮기기 S3, GCS 등 Object StorageFeature Store를 만들기 위해 Batch ETL 실행 Airflow 관련 추천 글 !!!!버킷플레이스 - Airflow 도입기라인 엔지니어링 - Airflow on Kubernetes쏘카 데이터 그룹 - Airflow와 함께한 데이터 환경 구축기Airflow Executors ExplainedSpecial MissionAirflow Local에 환경 설정하기학습을 1달 단위, 예측을 1일 단위로 하는 Airflow DAG 생성하기 Airflow 관련 추천 글 읽기BentoML머신러닝 디자인 패턴"
5,[Apache Kafka] Kafka REST Proxy란?,https://velog.velcdn.com/images/holicme7/profile/724be72c-2cb7-47f1-88cd-1474d73319b3/image.png,"Kafka공식 문서REST Proxy 란?Confluent에서 제공하는 Apache Kafka 클러스터를 위한 RESTful 인터페이스입니다.
네이티브 Kafka 프로토콜이나 Kafka Connect, Kafka Client를 사용하지 않고도 REST API 를 통해 카프카 클러스터에 메세지를 전달하고, 클러스터의 상태를 모니터링 및 관리할 수 있습니다.특징MetaData브로커, 토픽, 파티션, Config 등 카프카 클러스터에 대한 대부분의 메타 데이터를 GET 요청 방식으로 조회할 수 있습니다.ProducersAPI가 특정 토픽이나 파티션으로 들어오는 Produce Requests를 받아, 작은 프로듀서 풀을 통해 라우팅 시킵니다.Producer 인스턴스는 공유되기 때문에 요청별로 Producer 설정을 조정할 수 없습니다. 그러나, REST Proxy 에서 새로운 Producer 설정을 전달하여 전역적으로 설정을 조정할 수 있습니다.
예를 들어, 저장소 및 네트워크 오버헤드를 줄이기 위해 compression.type 옵션을 전달할 수 있습니다.
예를 들어, 저장소 및 네트워크 오버헤드를 줄이기 위해 compression.type 옵션을 전달할 수 있습니다.ConsumersConsumer는 Stateful아므로 특정 REST Proxy 인스턴스와 관련이 있습니다.Offset Commit은 자동으로 진행되거나 사용자가 명시적으로 요청할 수도 있습니다.현재 Consumer 당 하나의 스레드로 제한됩니다. 더 높은 처리량을 위해 여러개의 Consumer를 사용하세요.Consumer 인스턴스는 공유되지 않지만 기본 서버 리소스를 공유합니다. 그러므로, 제한된 설정 옵션들이 API를 통해 전달됩니다. 그러나  REST Proxt 설정에서 Consumer 설정을 전달하여 전역적으로 설정을 조정할 수 있습니다.Data FormatsREST Proxy는 JSON, Base64로 인코딩된 원시 바이트, Json 인코딩된 Avro, Protobuf 혹은 JSON 스키마를 사용하여 데이터를 읽고 쓸 수 있습니다.Avro, Protobuf, JSON Schema를 사용하면 스키마가 등록되고 Schema Registry에 대해 검증됩니다. REST Proxy Clusters and Load BalancingREST Prxoy는 함께 실행되는 다중 인스턴스에 부하가 분산되도록 설계되었습니다.다양한 로드밸런싱 메커니즘을 지원합니다. (Round Robin DNS, Service Discovery, Load Balancer)Admin operationsAPI v3 을 사용하여 토픽을 생성하거나 삭제하고, 토픽 설정을 업데이트하거나 초기화할 수 있습니다.아직 지원되지 않는 항목은 아래와 같습니다.다중 토픽 Produce 요청
현재 각 Produce 요청은 단일 토픽 혹은 토픽 파티션만 처리할 수 있습니다.
대신, 클라이언트는 필요한 경우 여러 요청으로 데이터를 분할할 수 있습니다.
현재 각 Produce 요청은 단일 토픽 혹은 토픽 파티션만 처리할 수 있습니다.대신, 클라이언트는 필요한 경우 여러 요청으로 데이터를 분할할 수 있습니다.요청에서 대부분의 Producer/Consumer 재정의 
몇가지 주요 재정의만 API에 노출됩니다. (단, 전역 재정의는 관리자가 설정할 수 있음)
몇가지 주요 재정의만 API에 노출됩니다. (단, 전역 재정의는 관리자가 설정할 수 있음)키-값에 대해 다른 직렬 변환기 허용 (serializer)
현재 REST Proxy는 Content-Type 헤더를 기반으로 직렬 변환기를 선택합니다. 결과적으로 키와 값에 대한 직렬 변환기는 이 디자인에서 동일해야 합니다.
현재 REST Proxy는 Content-Type 헤더를 기반으로 직렬 변환기를 선택합니다. 결과적으로 키와 값에 대한 직렬 변환기는 이 디자인에서 동일해야 합니다.[Apache Kafka] Kafka Streams 란?표준 메세징 프로토콜 정리 (AMQP, STOMP, MQTT)"
6,HBase,https://velog.velcdn.com/images%2Fthinkp92%2Fpost%2Ffdd7f62e-5ef3-4a03-a888-db5fefea1360%2FHBase%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.PNG,"소프트웨어 아키텍처HBase아파치 HBase는 Hadoop을 위한 공개 NoSQL 분산 데이터 베이스이다. 아파치 소프트웨어 재단에서 아파치 하둡 프로젝트 일부로서 개발되었으며 하둡의 분산 파일 시스템인 HDFS위에서 동작한다. 대량의 흩어져 있는 데이터 저장을 위해 falut-tolerent 방법을 제공하는 구글의 BigTable과 비슷한 기능을 한다.HBase 아키텍처HBase에 데이터 저장 과정1) 주키퍼를 통해 HTable의 기본 정보와 해당 HRegion의 위치 정보를 알아낸다.
2) 해당 정보를 기반으로 클라이언트가 직접 HRegionServer로 연결되어 MemStore에 데이터 저장
3) MemStore에 저장된 데이터는 특정 시점이 되면 HFile로 HDFS에 플러시 된다.HBase에서 데이터 읽는 과정1) 주키퍼를 통해 로우키(RowKey)에 해당하는 데이터의 위치 정보를 알아낸다.
2) 해당 HRegionServer의 MemStore에서 데이터를 가져옴으로써 디스크 I/O 최소화, 빠른응답속도
3)만일 데이터가 MemStore에서 플러시되어 존재히자 않으면 HFile 영역에서 데이터를 찾는다Referencehttps://ko.wikipedia.org/wiki/%EC%95%84%ED%8C%8C%EC%B9%98_HBase - 아파치 HBase
실무로 배우는 빅데이터 기술 - 김강원 지음/위키북스에스퍼(Esper)레디스(Redis)"
7,[데이터 처리] Hive,https://velog.velcdn.com/images/hyojinnnnnan/post/dfb1a3b8-409e-43ed-b122-31f209d593c3/image.png,"Data Engineering 기초등장 배경데이터를 처리하기 위해 사용하는 맵 리듀스는 사용자가 자바로 코드를 작성하여 하둡에 작업(Job)을 제출해야 하는 번거로움이 있었다. 이러한 점 때문에 하둡 상에서 SQL을 이용해 MapReduce를 처리할 수 있는 하이브가 탄생하게 되었다.설계 목표하이브의 목표는 복잡한 코드 대신 SQL을 사용하여 데이터를 편하게 처리 하는것이 목적이었고, 또한 Hive MetaStore를 통해 데이터베이스, 파티션, 테이블 등을 관리하는 카탈로그를 관리함에 따라 데이터를 쉽게 찾고 관리하는 것이 목표가 되었다. 하이브는 RDBMS가 아니라 데이터 웨어하우스 소프트웨어로서 하둡 상에서 SQL을 사용해 분산 스토리지(HDFS) 내에 있는 굉장히 큰 데이터를 분석하고 조회할 수 있는 프로젝트이다.하둡 상에서 SQL을 이용해 데이터를 처리하기 때문에 SQL On Hadoop 이라고 하며, 하이브에서 사용하는 SQL을 HiveQL 이라고 한다.Hive 아키텍처하이브의 아키텍처는 Hive Client, Hive Service, Compute(맵리듀스) & Storage(HDFS) 세 개의 계층으로 구성되어 있다.
Compute & Storage 은 결국 맵리듀스와 하둡인데, 하둡 위에서 맵리듀스를 SQL로 처리할 수 있게 만들기 위해 Hive Client와 Hive Service가 생기게 되었다. 하이브 클라이언트쿼리를 입력하는 어플리케이션이라고 생각하면 쉽다. 하이브 클라이언트를 통해 쿼리를 수행하게 된다. Thrift를 통해 하이브를 실행시킨다. Thrift는 다양한 언어를 지원하는 RPC(Remote Procedure Call)로서, 언어가 달라도 원격 제어를 통해 별다른 코딩 없이 함수나 프로시저를 실행할 수 있게 하는 통신 기술 중 하나이다. 하이브 서버가 Thrift 기반으로 만들어져있기 때문에 다양한 프로그래밍 언어에서 Thrift 언어를 통해 하이브 서버에 요청할 수 있다. 또한 하이브는 JDBC, ODBC 드라이브를 지원하여 쿼리를 연결할 수 있다. 하이브 클라이언트는 하이브 서버에 SQL을 전달한다. 하이브 서비스모든 클라이언트는 하이브 서버에 요청(SQL)을 제출한다. 하이브 서버에서는 하이브를 실행하기 위해 드라이버에 요청을 전달하게 된다. 드라이버는 실제 쿼리를 받아 작업을 수행하게 되며, 이 작업은 Hadoop MapReduce를 통해 실제 수행된다. 드라이버는 SQL을 하둡의 맵리듀스 Job으로 바꿔주는 역할을 하게 된다.또한 드라이버를 통해 실행된 작업의 내역은  메타스토어에 기록이 된다. MetaStore에는 하이브 테이블에 대한 메타 정보를 가지며, 테이블의 스키마, 테이블이 저장된 위치 정보로 관리된다. 또한 파티션 정보를 통해 드라이버가 클러스터에 분산된 다양한 데이터 셋의 진행상황을 추적할 수도 있다. 드라이버는 요청받은 쿼리를 실행하기 위해 하둡에 맵리듀스 Job을 제출하게된다. 드라이버는 내부적으로 Compiler, Optimizer, Executor가 존재한다.하이브 서버에 의해 SQL을 받아오면컴파일러가 쿼리를 확인하고 분석한다. Optimizer는 분석된 이 쿼리를 최적화 시킨다.Excutor는 실제 Task를 수행한다. 연산과 저장소Hive 데이터 처리 과정Hive 데이터 모델링Hive와 RDBMS의 차이점Hive 사용 시 고려사항Hive 설치하기[데이터 처리] Hadoop MapReduce[데이터 처리] Apache Spark"
8,컴퓨터망 18) FTP TFTP,https://velog.velcdn.com/images/zh025700/post/fa241eda-5505-48a5-a541-8e3090d7e6cb/image.png,"컴퓨터네트워크18. File Transfer: FTP이 장에서는 파일 전송과 관련된 두 가지 프로토콜인 FTP(File Transfer Protocol)와 TFTP(Trivial File Transfer Protocol)에 대해 설명한다FTP(File Transfer Protocol)는 한 호스트에서 다른 호스트로 파일을 복사하기 위해 TCP/IP에서 제공하는 표준 메커니즘이다
한 시스템에서 다른 시스템으로 파일을 전송하는 것이 간단하고 간단해 보이지만 먼저 몇 가지 문제를 해결해야 한다
예를 들어, 두 시스템이 서로 다른 파일 이름 규칙을 사용할 수 있다
두 시스템은 텍스트와 데이터를 표현하는 방식이 다를 수 있다
두 시스템은 서로 다른 디렉토리 구조를 가질 수 있다
이러한 모든 문제는 FTP를 통해 해결했다FTP는 TCP 서비스를 사용한다
두 개의 TCP 연결이 필요하다
well known 포트 21은 control connection에 사용되고 포트 20은 data connection에 사용된다
FTP는 호스트 간에 두 가지 연결을 설정한다는 점에서 다른 클라이언트-서버 응용 프로그램과 다르다
FTP는 호스트 간에 두 가지 연결을 설정한다는 점에서 다른 클라이언트-서버 응용 프로그램과 다르다
하나의 연결은 데이터 전송에 사용되고 다른 하나는 제어 정보(명령 및 응답)에 사용된다

명령과 데이터 전송을 분리하여 효율적이다

하나의 연결은 데이터 전송에 사용되고 다른 하나는 제어 정보(명령 및 응답)에 사용된다명령과 데이터 전송을 분리하여 효율적이다
control connection은 매우 간단한 통신 규칙을 사용한다

한 번에 한 줄의 명령이나 한 줄의 응답만 전달하면 된다

control connection은 매우 간단한 통신 규칙을 사용한다한 번에 한 줄의 명령이나 한 줄의 응답만 전달하면 된다
반면에 data connection은 전송되는 다양한 데이터 유형으로 인해 더 복잡한 규칙이 필요하다
반면에 data connection은 전송되는 다양한 데이터 유형으로 인해 더 복잡한 규칙이 필요하다
FTP는 두 개의 well known TCP 포트를 사용한다

포트 21은 control connection에 사용되고 포트 20은 data connection에 사용된다

FTP는 두 개의 well known TCP 포트를 사용한다포트 21은 control connection에 사용되고 포트 20은 data connection에 사용된다그림은 FTP의 기본 모델을 보여준다클라이언트 구성요소사용자 인터페이스제어 프로세스데이터 전송 프로세스서버 구성요소제어 프로세스서버 데이터 전송 프로세스control connection은 제어 프로세스 간에 이루어진다
data connection은 데이터 전송 프로세스 간에 이루어진다Control connectioncontrol connection에는 두 단계가 있다서버는 포트 21에서 passive open을 실행하고 클라이언트를 기다린다클라이언트는 임시 포트를 사용하고 active open를 실행연결은 전체 프로세스 동안 유지된다Data Connectiondata connection은 서버 사이트에서 well known 포트 20을 사용한다서버가 아닌 클라이언트가 임시 포트를 사용하여 passive open을 실행 클라이언트가 파일 전송을 원하기 때문에 passive open을 클라가 한다클라이언트는 PORT 명령을 사용하여 이 포트 번호를 서버에 보낸다서버는 포트 번호를 수신하고 well known 포트 20과 수신된 임시 포트 번호를 사용하여 active open을 실행초기 data connection 단계이다
나중에 PASV 명령을 사용하면 이러한 단계가 변경이 된다Communication서로 다른 컴퓨터에서 실행되는 FTP 클라이언트와 서버는 서로 통신해야 한다
이 두 컴퓨터는 다른 운영 체제, 다른 문자 집합, 다른 파일 구조 및 다른 파일 형식을 사용할 수 있다
FTP는 이 이질성을 호환 가능하게 만들어야 한다Communication over control connectionFTP는 TELNET 또는 SMTP와 동일한 접근 방식을 사용한다
NVT ASCII 문자 집합을 사용한다
NVT ASCII 문자 집합을 사용한다커뮤니케이션은 command와 reponse를 통해 이루어진다
한 번에 하나의 명령(또는 응답)을 보내기 때문에 control connection에 적합하다
한 번에 하나의 명령(또는 응답)을 보내기 때문에 control connection에 적합하다Communication over data connectiondata connection을 통해 파일을 전송한다클라이언트는 전송할 파일 유형, 데이터 구조 및 전송 모드를 정의해야한다
data connection을 통해 파일을 전송하기 전에 control connection을 통해 데이터 전송을 준비한다
data connection을 통해 파일을 전송하기 전에 control connection을 통해 데이터 전송을 준비한다이질성 문제는 파일 유형, 데이터 구조 및 전송 모드의 세 가지 통신 속성을 정의하여 해결된다Command processingFTP는 control connection을 사용하여 클라이언트 제어 프로세스와 서버 제어 프로세스 간의 통신을 설정한다
이 통신 동안 명령은 클라이언트에서 서버로 전송되고 응답은 서버에서 클라이언트로 전송된다Access commands이 명령을 사용하면 사용자가 원격 시스템에 액세스할 수 있다File management commands이 명령을 통해 사용자는 원격 컴퓨터의 파일 시스템에 액세스할 수 있다이를 통해 사용자는 디렉터리 구조를 탐색하고, 새 디렉터리를 만들고, 파일을 삭제하는 등의 작업을 수행할 수 있다data formatiting commands이 명령을 통해 사용자는 데이터 구조, 파일 유형 및 전송 모드를 정의할 수 있다정의된 형식은 파일 전송 명령에서 사용됩니다.Port defining commands이 명령은 클라이언트에서 data connection을 위한 포트 번호를 정의한다두 가지 방법이 있다
첫 번째 방법
PORT 명령을 사용하여 클라이언트가 임시 포트 번호를 서버에 보낼 수 있다.
두 번째 방법
PASV 명령을 사용하여 클라이언트가 서버에 먼저 포트 번호를 선택하도록 요청

첫 번째 방법PORT 명령을 사용하여 클라이언트가 임시 포트 번호를 서버에 보낼 수 있다.두 번째 방법
PASV 명령을 사용하여 클라이언트가 서버에 먼저 포트 번호를 선택하도록 요청
PASV 명령을 사용하여 클라이언트가 서버에 먼저 포트 번호를 선택하도록 요청File transfer commands이 명령을 사용하면 실제로 사용자가 파일을 전송할 수 있다Miscellaneous commands이 명령은 클라이언트 사이트의 FTP 사용자에게 정보를 전달Response모든 FTP 명령은 적어도 하나의 response를 생성한다
File transfer파일 전송은 control connection을 통해 전송된 명령 후 data connection을 통해 발생한다
FTP에서의 파일 전송은 아래 세 가지 중 하나를 의미한다서버에서 클라이언트로 파일을 복사(다운로드)
이것을 retrieving a file이라고 한다
RETR 명령에 의해 수행
이것을 retrieving a file이라고 한다RETR 명령에 의해 수행클라이언트에서 서버로 파일을 복사(업로드)
이것을 파일 저장이라고 한다
STOR 명령의 감독하에 수행
이것을 파일 저장이라고 한다STOR 명령의 감독하에 수행디렉토리 또는 파일 이름 목록이 서버에서 클라이언트로 전송
이것은 LIST 명령의 의해 수행
이것은 LIST 명령의 의해 수행Ex. retrieving a list포트 21에 대한 control connection이 생성된 후 FTP 서버는 control connection에 대해 220(서비스 준비) 응답을 보냄클라이언트가 USER 명령을 보냄서버는 331로 응답(사용자 이름은 OK이고 암호가 필요)클라이언트가 PASS 명령을 보냄서버는 230으로 응답(사용자 로그인은 OK)클라이언트는 data connection을 위해 임시 포트에서 passive open을 실행하고 PORT 명령을 전송하여(control connection을 통해) 이 포트 번호를 서버에 제공서버는 이 시점에서 연결을 열지 않지만 포트 20(서버 측)과 클라이언트에서 수신한 임시 포트 간의 data connection에서 active open를 발행할 준비를 한다 응답 150을 보낸다(data connection이 곧 열림)클라이언트는 LIST 메시지를 보냄이제 서버는 125로 응답하고 data connection을 연다그런 다음 서버는 data connection에서 파일 또는 디렉터리 목록(파일로)을 보낸다 전체 목록(파일)이 전송되면 서버는 control connection을 통해 226(data connection 닫기)으로 응답이제 클라이언트는 두 가지 선택을 할 수 있다 QUIT 명령을 사용하여 control connection 닫기를 요청하거나 다른 명령을 보내 다른 활동을 시작할 수 있다(결국 다른 data connection을 열 수 있음)QUIT 명령을 받은 서버는 221(서비스 닫기)로 응답한 다음 control connection을 닫는다Ex. file stored그림은 이미지(바이너리) 파일이 저장되는 방법을 보여준다포트 21에 대한 control connection이 생성된 후 FTP 서버는 control connection에 대해 220(서비스 준비) 응답을 보낸다클라이언트가 USER 명령을 보낸다서버는 331로 응답한다(사용자 이름은 정상이며 암호가 필요)클라이언트가 PASS 명령을 보낸다서버는 230으로 응답(사용자 로그인은 OK)클라이언트는 data connection을 위해 임시 포트에서 passive open을 실행하고 PORT 명령을 전송하여(control connection을 통해) 이 포트 번호를 서버에 제공서버는 이 시점에서 연결을 열지 않지만 포트 20(서버 측)과 클라이언트에서 수신한 임시 포트 간의 data connection에서 active open을 발행할 준비를 한다 응답 150을 보냅니다(data connection이 곧 열림).클라이언트가 TYPE 명령을 보냄서버는 응답 200(명령 OK)으로 응답클라이언트가 STRU 명령을 보냄서버는 200(명령 OK)으로 응답클라이언트가 STOR 명령을 보냄서버는 data connection을 열고 응답 250을 보냄클라이언트는 data connection에서 파일을 보냄 전체 파일이 전송된 후 data connection이 닫힘 data connection을 닫는 것은 파일 끝을 의미서버는 control connection에 대한 응답 226을 보냄클라이언트가 QUIT 명령을 보내거나 다른 명령을 사용하여 다른 파일을 전송하기 위해 다른 data connection을 연다 이 예에서는 QUIT 명령이 전송서버는 221(서비스 종료)로 응답하고 control connection을 닫음FTP 프로토콜의 모든 기능이 필요 없이 단순히 파일을 복사해야 하는 경우가 있다
예를 들어 디스크가 없는 워크스테이션이나 라우터가 부팅될 때 부트스트랩과 구성 파일을 다운로드해야 한다
여기서 우리는 FTP에서 제공되는 모든 정교함을 필요로 하지 않는다
파일을 빠르게 복사하는 프로토콜만 있으면 된다TFTP(Trivial File Transfer Protocol)는 이러한 유형의 파일 전송을 위해 설계되었다TFTP는 UDP를 사용하고 well known 포트 69를 사용한다MessageTFTP 메시지에는 그림과 같이 RRQ, WRQ, DATA, ACK, ERROR의 5가지 유형이 있다Use of TFTP with DHCPTFTP는 보안이 큰 문제가 되지 않는 기본 파일 전송에 매우 유용하다주요 응용 프로그램은 DHCP와 함께 사용됩니다TFTP는 적은 양의 메모리만 필요로 하며 UDP 및 IP 서비스만 사용한다전원이 켜진 기기는 DHCP 클라이언트를 사용하여 DHCP 서버에서 구성 파일의 이름을 가져온다
이후 스테이션은 파일 이름을 TFTP 클라이언트에 전달하여 TFTP 서버에서 구성 파일의 내용을 가져온다
이후 스테이션은 파일 이름을 TFTP 클라이언트에 전달하여 TFTP 서버에서 구성 파일의 내용을 가져온다컴퓨터망 17) TELNET, SSH컴퓨터망 19) www HTTP"
9,SP - 8.1 Virtual Memory Concepts,https://velog.velcdn.com/images/junttang/post/3b5d7eb6-6b64-4360-8f51-afd647cbb942/image.png,"SystemProgramming  대망의 System Programming 연재 마지막 포스팅이다. 마지막 개념은 그동안 계속 언급만 해왔던 Virtual Memory이다. 이는 Computer Architecture를 공부했다면 어느정도 이미 알고 있을 수 있는 내용이지만, SP 연재가 끝나면 새롭게 진행할 예정인 'OS(운영체제)' 연재를 미리 간단히 준비하고자, 본 포스팅을 통해 간단히 다뤄보고자 한다. 너무 Deep하게 다루진 않을 것이다.Address Space""System에서 Process가 보는 Address Space는 무엇인가?""이 질문에 대한 답에 따라 System을 크게 두 가지로 분류할 수 있다.
Process가 보는 Address Space가 Physical Address Space인 System


Embedded Microcontroller가 대표적이다. 자동차, 엘리베이터 등에 들어가는 소형 CPU를 생각하면 된다.

CPU가 메인 메모리에 접근할 때, CPU가 Addressing하는 주소는 Physical Address이다.



즉, 가상 메모리를 지원하지 않는 시스템이 모두 여기에 들어간다.


Process가 보는 Address Space가 Physical Address Space인 System
Embedded Microcontroller가 대표적이다. 자동차, 엘리베이터 등에 들어가는 소형 CPU를 생각하면 된다.

CPU가 메인 메모리에 접근할 때, CPU가 Addressing하는 주소는 Physical Address이다.

Embedded Microcontroller가 대표적이다. 자동차, 엘리베이터 등에 들어가는 소형 CPU를 생각하면 된다.CPU가 메인 메모리에 접근할 때, CPU가 Addressing하는 주소는 Physical Address이다.
즉, 가상 메모리를 지원하지 않는 시스템이 모두 여기에 들어간다.
즉, 가상 메모리를 지원하지 않는 시스템이 모두 여기에 들어간다.Process가 보는 Address Space가 Virtual Address Space인 System

일반적인 현대식 컴퓨터가 모두 해당한다.


CPU가 메인 메모리에 접근할 때, CPU가 Addressing하는 주소는 Virtual Address이다.


물론, 실제 메모리는 당연 Physical Address이다.

CPU에 들어있는 MMU(Memory Management Unit)가 Virtual Memory to Physical Memory 변환을 수행한다.





일반적인 현대식 컴퓨터가 모두 해당한다.


CPU가 메인 메모리에 접근할 때, CPU가 Addressing하는 주소는 Virtual Address이다.


물론, 실제 메모리는 당연 Physical Address이다.

CPU에 들어있는 MMU(Memory Management Unit)가 Virtual Memory to Physical Memory 변환을 수행한다.



일반적인 현대식 컴퓨터가 모두 해당한다.
CPU가 메인 메모리에 접근할 때, CPU가 Addressing하는 주소는 Virtual Address이다.
CPU가 메인 메모리에 접근할 때, CPU가 Addressing하는 주소는 Virtual Address이다.
물론, 실제 메모리는 당연 Physical Address이다.

CPU에 들어있는 MMU(Memory Management Unit)가 Virtual Memory to Physical Memory 변환을 수행한다.

물론, 실제 메모리는 당연 Physical Address이다.CPU에 들어있는 MMU(Memory Management Unit)가 Virtual Memory to Physical Memory 변환을 수행한다.우측 Modern Desktop Computer를 보면, 1234번지라는 가상 메모리 주소가 4라는 물리 메모리 주소로 변환되고 있다.
누구에 의해서? 그렇다. MMU에 의해서!
누구에 의해서? 그렇다. MMU에 의해서!  Address Space는 아래와 같이 크게 세 가지 유형으로 분류할 수 있다.
Linear Address Space : 연속적인 Non-Negative Integer Address로 이루어진 'Ordered Set'이다.

{0,1,2,3,4,...}

Linear Address Space : 연속적인 Non-Negative Integer Address로 이루어진 'Ordered Set'이다.{0,1,2,3,4,...}
Virtual Address Space : 우리가 몇 비트로 가상 주소를 표현하느냐에 따라 범위가 달라진다. n-Bits라 하면, 2^n 크기의 가상 메모리 주소가 마련된다.

{0,1,2,3,4,...,n-2,n-1}
n-Bits!


Virtual Address Space : 우리가 몇 비트로 가상 주소를 표현하느냐에 따라 범위가 달라진다. n-Bits라 하면, 2^n 크기의 가상 메모리 주소가 마련된다.{0,1,2,3,4,...,n-2,n-1}
n-Bits!
n-Bits!
Physical Address Space : 우리가 몇 비트로 물리 주소를 설계하느냐에 따라 범위가 달라진다. m-Bits라 하면, 2^m 크기의 물리(실제) 메모리 주소가 마련된다.

{0,1,2,3,4,...,m-2,m-1}
m-Bits!



일반적으로 Virtual Memory가 Physical Memory보다 크기가 크게 설정된다.

n > m


Physical Address Space : 우리가 몇 비트로 물리 주소를 설계하느냐에 따라 범위가 달라진다. m-Bits라 하면, 2^m 크기의 물리(실제) 메모리 주소가 마련된다.{0,1,2,3,4,...,m-2,m-1}
m-Bits!
m-Bits!일반적으로 Virtual Memory가 Physical Memory보다 크기가 크게 설정된다.n > mVirtual MemoryWhy? (Advantages of VM)  그냥 Linear Address로 간편하게 사용하면 될 것을, 왜 굳이 Virtual과 Physical로 Memory를 구분하는 것일까? Virtual Memory를 도입함으로써 우리가 얻을 수 있는 이득이 무엇일까?Main Memory를 효율적으로 사용할 수 있다.
DRAM을 효과적으로 사용할 수 있다. 무슨 말이냐면, 가상 주소의 일부만 DRAM에 올린다는 것이다. ★
Process는 가상 메모리 영역으로 'Data-Code-Heap-Stack' Segments를 가진다고 했다. Program이 Memory에 Load되어 Process가 됐다고 해보자. 과연 이 Process는 얼마만큼의 메모리 공간을 접근할 것인가? 그렇다. 생각보다 그리 많은 공간을 점유하진 않을 것이다. Virtual Memory를 도입하면, Process에 대한 Memory 영역을 가상 메모리 영역으로 잡고, 그 중 실제로 사용할 (것 같은) 녀석들만, 즉, 일부만 Main Memory에 올리는 것이다.


Process에 대한 모든 메모리 영역을 Main Memory에 올리는 것은 공간 효율 관점에서 비효율적이다.

즉, DRAM을 낭비하지 말고, 필요한 것들만 올리자는 것이다.

DRAM(Main Memory)을 마치 Virtual Memory에 대한 Cache처럼 사용하는 것이다. 나머지 '덜 중요한' 부분은 Disk로 올리는 것이다. ★★★★★


DRAM을 효과적으로 사용할 수 있다. 무슨 말이냐면, 가상 주소의 일부만 DRAM에 올린다는 것이다. ★
Process는 가상 메모리 영역으로 'Data-Code-Heap-Stack' Segments를 가진다고 했다. Program이 Memory에 Load되어 Process가 됐다고 해보자. 과연 이 Process는 얼마만큼의 메모리 공간을 접근할 것인가? 그렇다. 생각보다 그리 많은 공간을 점유하진 않을 것이다. Virtual Memory를 도입하면, Process에 대한 Memory 영역을 가상 메모리 영역으로 잡고, 그 중 실제로 사용할 (것 같은) 녀석들만, 즉, 일부만 Main Memory에 올리는 것이다.
Process는 가상 메모리 영역으로 'Data-Code-Heap-Stack' Segments를 가진다고 했다. Program이 Memory에 Load되어 Process가 됐다고 해보자. 과연 이 Process는 얼마만큼의 메모리 공간을 접근할 것인가? 그렇다. 생각보다 그리 많은 공간을 점유하진 않을 것이다. Virtual Memory를 도입하면, Process에 대한 Memory 영역을 가상 메모리 영역으로 잡고, 그 중 실제로 사용할 (것 같은) 녀석들만, 즉, 일부만 Main Memory에 올리는 것이다.Process에 대한 모든 메모리 영역을 Main Memory에 올리는 것은 공간 효율 관점에서 비효율적이다.즉, DRAM을 낭비하지 말고, 필요한 것들만 올리자는 것이다.DRAM(Main Memory)을 마치 Virtual Memory에 대한 Cache처럼 사용하는 것이다. 나머지 '덜 중요한' 부분은 Disk로 올리는 것이다. ★★★★★Memory Management를 간소화할 수 있다.
모든 Process가 동일한(Uniform) Linear Address Space를 가질 수 있게 만들어준다. 모두가 동일한 크기와 순서의 메모리 공간을 상상할 수 있게 하는 것이다. ★★★
만일, VM이 없다면, 각 프로세스가 서로 다른 시각을 가져야한다.

모든 Process가 동일한(Uniform) Linear Address Space를 가질 수 있게 만들어준다. 모두가 동일한 크기와 순서의 메모리 공간을 상상할 수 있게 하는 것이다. ★★★
만일, VM이 없다면, 각 프로세스가 서로 다른 시각을 가져야한다.
만일, VM이 없다면, 각 프로세스가 서로 다른 시각을 가져야한다.Address Space를 Isolate할 수 있다. 즉, 어떤 Process가 다른 Process의 메모리 영역을 침범할 수 없다. ★
마찬가지로, Application이 Kernel Information과 Kernel Code 부분을 접근할 수 없게 할 수 있다. ★
마찬가지로, Application이 Kernel Information과 Kernel Code 부분을 접근할 수 없게 할 수 있다. ★Caching Purpose  VM을 Caching을 위한 Tool로 바라볼 수 있다. Process가 실제 Memory에 올라가고 구동되면, 개념적인 관점에서 Virtual Memory는 N개의 연속된 Byte Array이다.
Process가 바라보는 '전체 Virtual Memory'는 Disk에 Load된다. (당연)

이러한 Virtual Memory는 모든 Process에 대해 일일이 존재한다고 했다.

Process가 바라보는 '전체 Virtual Memory'는 Disk에 Load된다. (당연)이러한 Virtual Memory는 모든 Process에 대해 일일이 존재한다고 했다.
Disk에 있는 데이터 중 일부(Important Data)는 DRAM, Main Memory에 올라가는 것이다.

정확히는, Virtual Page 중 Active Page들만 Main Memory로 올라가는 것이다. ★


즉, Main Memory가 마치 Virtual Memory에 대한 Cache처럼 작용하는 것이다. ★

Disk에 있는 데이터 중 일부(Important Data)는 DRAM, Main Memory에 올라가는 것이다.정확히는, Virtual Page 중 Active Page들만 Main Memory로 올라가는 것이다. ★즉, Main Memory가 마치 Virtual Memory에 대한 Cache처럼 작용하는 것이다. ★
위 그림에서, 각 네모칸은 Page이다. 보통 이 Page는 4KB나 8KB 정도로 잡는다.

가상 메모리는 이러한 Page들의 연속된 배열이다.

위 그림에서, 각 네모칸은 Page이다. 보통 이 Page는 4KB나 8KB 정도로 잡는다.가상 메모리는 이러한 Page들의 연속된 배열이다.
Physical Memory 역시 마찬가지로 Page 단위로 구성된 배열이다.

Virtual Page와 Physical Page가 Mapping된다. ★★★

Physical Memory 역시 마찬가지로 Page 단위로 구성된 배열이다.Virtual Page와 Physical Page가 Mapping된다. ★★★Virtual Memory의 일부 Page들이 Cached로서, Main Memory에 올라가는 것이다.이러한 관점에서, Main Memory, 즉, Physical Memory를 DRAM Cache라고 부른다. ★Architectural View프로그램이 특정 데이터에 접근할 때, 가상 메모리 주소를 확인한다.
이 가상 메모리 주소를 'Page Number'와 'Offset'으로 쪼갤 수 있다.

Page Number를 기준으로, TLB(Translation Lookaside Buffer)를 확인한다. TLB는 Page Table에 대한 Cache이다.


TLB Hit를 하면, 즉, TLB에서 찾고자 하는 Page Number를 찾으면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!



TLB Miss를 하면, 즉, TLB에 찾고자 하는 Page Number가 없으면, Page Table을 훑는다. 여기서 Target Page Number가 발견되면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!



Page Table에도 Target이 없으면, 이는 초반 포스팅에서 언급했던 'Page Fault'이다.

Secondary Memory인 Disk에서 찾고자 하는 Data를 추출해 Main Memory, DRAM으로 Load한다. 
이 New Physical Address를 기반으로 Page Table, TLB를 LRU 등의 Replacement 기법을 적용해 업데이트한다.
Fault를 야기한 Instruction을 다시 수행해 Data를 Fetch한다.
Data가 Load된다!








이 가상 메모리 주소를 'Page Number'와 'Offset'으로 쪼갤 수 있다.

Page Number를 기준으로, TLB(Translation Lookaside Buffer)를 확인한다. TLB는 Page Table에 대한 Cache이다.


TLB Hit를 하면, 즉, TLB에서 찾고자 하는 Page Number를 찾으면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!



TLB Miss를 하면, 즉, TLB에 찾고자 하는 Page Number가 없으면, Page Table을 훑는다. 여기서 Target Page Number가 발견되면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!



Page Table에도 Target이 없으면, 이는 초반 포스팅에서 언급했던 'Page Fault'이다.

Secondary Memory인 Disk에서 찾고자 하는 Data를 추출해 Main Memory, DRAM으로 Load한다. 
이 New Physical Address를 기반으로 Page Table, TLB를 LRU 등의 Replacement 기법을 적용해 업데이트한다.
Fault를 야기한 Instruction을 다시 수행해 Data를 Fetch한다.
Data가 Load된다!








Page Number를 기준으로, TLB(Translation Lookaside Buffer)를 확인한다. TLB는 Page Table에 대한 Cache이다.


TLB Hit를 하면, 즉, TLB에서 찾고자 하는 Page Number를 찾으면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!



TLB Miss를 하면, 즉, TLB에 찾고자 하는 Page Number가 없으면, Page Table을 훑는다. 여기서 Target Page Number가 발견되면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!



Page Table에도 Target이 없으면, 이는 초반 포스팅에서 언급했던 'Page Fault'이다.

Secondary Memory인 Disk에서 찾고자 하는 Data를 추출해 Main Memory, DRAM으로 Load한다. 
이 New Physical Address를 기반으로 Page Table, TLB를 LRU 등의 Replacement 기법을 적용해 업데이트한다.
Fault를 야기한 Instruction을 다시 수행해 Data를 Fetch한다.
Data가 Load된다!






Page Number를 기준으로, TLB(Translation Lookaside Buffer)를 확인한다. TLB는 Page Table에 대한 Cache이다.
TLB Hit를 하면, 즉, TLB에서 찾고자 하는 Page Number를 찾으면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!

TLB Hit를 하면, 즉, TLB에서 찾고자 하는 Page Number를 찾으면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.Data가 Load된다!
TLB Miss를 하면, 즉, TLB에 찾고자 하는 Page Number가 없으면, Page Table을 훑는다. 여기서 Target Page Number가 발견되면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.

Data가 Load된다!

TLB Miss를 하면, 즉, TLB에 찾고자 하는 Page Number가 없으면, Page Table을 훑는다. 여기서 Target Page Number가 발견되면, 이를 토대로 변환하여 Physical Memory Address를 얻는다.Data가 Load된다!
Page Table에도 Target이 없으면, 이는 초반 포스팅에서 언급했던 'Page Fault'이다.

Secondary Memory인 Disk에서 찾고자 하는 Data를 추출해 Main Memory, DRAM으로 Load한다. 
이 New Physical Address를 기반으로 Page Table, TLB를 LRU 등의 Replacement 기법을 적용해 업데이트한다.
Fault를 야기한 Instruction을 다시 수행해 Data를 Fetch한다.
Data가 Load된다!




Page Table에도 Target이 없으면, 이는 초반 포스팅에서 언급했던 'Page Fault'이다.Secondary Memory인 Disk에서 찾고자 하는 Data를 추출해 Main Memory, DRAM으로 Load한다. 
이 New Physical Address를 기반으로 Page Table, TLB를 LRU 등의 Replacement 기법을 적용해 업데이트한다.
Fault를 야기한 Instruction을 다시 수행해 Data를 Fetch한다.
Data가 Load된다!


이 New Physical Address를 기반으로 Page Table, TLB를 LRU 등의 Replacement 기법을 적용해 업데이트한다.
Fault를 야기한 Instruction을 다시 수행해 Data를 Fetch한다.
Data가 Load된다!

Fault를 야기한 Instruction을 다시 수행해 Data를 Fetch한다.
Data가 Load된다!
Data가 Load된다!(여기서, TLB고, Page Table이고, 이런 개념이 중요한 것이 아니다. 이는 Computer Architecture에서 배우는 내용이다. 중요한 것은, DRAM이 Virtual Memory의 Cache처럼 작용하고 있는 점이 중요하다. aka 'DRAM Cache' ★★★)DRAM Cache  DRAM Cache, 줄여서 DRAM은 SRAM(for 'Real(General)' Cache)보다 10배 느리다. 그리고 Disk는 이러한 DRAM보다 10,000배 느리다.
DRAM의 각 Page(Block)는 일반적으로 4KB 크기를 가진다.

Virtual Page 역시 마찬가지로, 같은 크기를 가진다.

DRAM의 각 Page(Block)는 일반적으로 4KB 크기를 가진다.Virtual Page 역시 마찬가지로, 같은 크기를 가진다.
Fully Associative : Virtual Page를 Physical Page로 Mapping할 때, Fully Associative 방식으로 맵핑된다.

Virtual Page가 어떤 Physical Page에도 맵핑될 수 있다. (Fully Associative) ★
Large Mapping Function이 필요하다. 
VP가 어떤 PP에 맵핑되는지를 관장함.



Fully Associative : Virtual Page를 Physical Page로 Mapping할 때, Fully Associative 방식으로 맵핑된다.Virtual Page가 어떤 Physical Page에도 맵핑될 수 있다. (Fully Associative) ★
Large Mapping Function이 필요하다. 
VP가 어떤 PP에 맵핑되는지를 관장함.

Large Mapping Function이 필요하다. 
VP가 어떤 PP에 맵핑되는지를 관장함.
VP가 어떤 PP에 맵핑되는지를 관장함.
Cost가 높은, 정교한(Sophisticated) Replacement Algorithm이 필요하다.

Disk에서 새로운 데이터를 Fetch하고자 하는데, DRAM이 꽉차 있으면, 어떤 Page를 Evict할 지 특정 알고리즘을 토대로 결정한다. ex) LRU(Least Recently Used), MRU, etc.
Expensive! ~> 그래서 HW적으로 구현함. Overhead를 최대한 낮추고자!


Cost가 높은, 정교한(Sophisticated) Replacement Algorithm이 필요하다.Disk에서 새로운 데이터를 Fetch하고자 하는데, DRAM이 꽉차 있으면, 어떤 Page를 Evict할 지 특정 알고리즘을 토대로 결정한다. ex) LRU(Least Recently Used), MRU, etc.
Expensive! ~> 그래서 HW적으로 구현함. Overhead를 최대한 낮추고자!
Expensive! ~> 그래서 HW적으로 구현함. Overhead를 최대한 낮추고자!
Write-Through 방식 대신 Write-Back 방식이 사용된다. (Synchronization)

Write 시 Cache만 Update하고, 나중에 Cache의 Page를 Evict할 때, Dirty Bit를 확인해 수정 유무를 판단 후, 수정이 있는 Page만 Disk에 Update한다.

Write-Through 방식 대신 Write-Back 방식이 사용된다. (Synchronization)Write 시 Cache만 Update하고, 나중에 Cache의 Page를 Evict할 때, Dirty Bit를 확인해 수정 유무를 판단 후, 수정이 있는 Page만 Disk에 Update한다.  DRAM과 Disk(or Virtual Memory) 관계, L3 Cache와 DRAM 관계, Li-1 Cache와 Li Cache 사이에서 위의 개념이 모두 공통적으로 적용된다(Associativity 제외). 자세한 Detail은 SP에서는 다룰 이유가 없다. 가볍게 읽어가라. ★★★Page Table  Page Table은 'PTE(Page Table Entry)'로 구성된 배열로, Virtual Page를 Physical Page에 맵핑하는 역할을 수행한다.Page Table은 Process마다 DRAM에 자료구조로서 존재한다. ★
OS가 이를 관리한다. (in Kernel Context)
프로그램이 10개가 돌아가고 있으면, 그들에 대응하는 10개의 Page Table을 가지고 있는 것이다. ★

OS가 이를 관리한다. (in Kernel Context)
프로그램이 10개가 돌아가고 있으면, 그들에 대응하는 10개의 Page Table을 가지고 있는 것이다. ★
프로그램이 10개가 돌아가고 있으면, 그들에 대응하는 10개의 Page Table을 가지고 있는 것이다. ★
가상 메모리를 물리 메모리로 맵핑하는 Mapping Function이 있다고 위에서 언급했다. 그 역할을 수행하는 것이 바로 Page Table인 것이다.


""Page Table is a Translator!""


어떤 Virtual Page가 어떤 Physical Page와 Mapping되어 있는지 정보를 관리한다. ★


맵핑 정보를 각 PTE에 저장한다. 


Virtual Memory의 Index가 곧 PTE Index라고 볼 수 있다. ★


앞서, VP는 Cached, Uncached, Unallocated 등으로 구분됐는데, Cached인 VP들이 다 DRAM에 맵핑되어 있는 것을 의미한다. ★★★

TLB는 여기선 고려 x





가상 메모리를 물리 메모리로 맵핑하는 Mapping Function이 있다고 위에서 언급했다. 그 역할을 수행하는 것이 바로 Page Table인 것이다.
""Page Table is a Translator!""
""Page Table is a Translator!""
어떤 Virtual Page가 어떤 Physical Page와 Mapping되어 있는지 정보를 관리한다. ★
어떤 Virtual Page가 어떤 Physical Page와 Mapping되어 있는지 정보를 관리한다. ★
맵핑 정보를 각 PTE에 저장한다. 


Virtual Memory의 Index가 곧 PTE Index라고 볼 수 있다. ★


앞서, VP는 Cached, Uncached, Unallocated 등으로 구분됐는데, Cached인 VP들이 다 DRAM에 맵핑되어 있는 것을 의미한다. ★★★

TLB는 여기선 고려 x



맵핑 정보를 각 PTE에 저장한다. 
Virtual Memory의 Index가 곧 PTE Index라고 볼 수 있다. ★
Virtual Memory의 Index가 곧 PTE Index라고 볼 수 있다. ★
앞서, VP는 Cached, Uncached, Unallocated 등으로 구분됐는데, Cached인 VP들이 다 DRAM에 맵핑되어 있는 것을 의미한다. ★★★

TLB는 여기선 고려 x

앞서, VP는 Cached, Uncached, Unallocated 등으로 구분됐는데, Cached인 VP들이 다 DRAM에 맵핑되어 있는 것을 의미한다. ★★★TLB는 여기선 고려 x
Page Hit : Reference to VM word that is in physical memory 

is equal to 'DRAM Cache Hit'
Valid Bit of PTE is 1 ★

Cached인 VM!

Page Hit : Reference to VM word that is in physical memory is equal to 'DRAM Cache Hit'
Valid Bit of PTE is 1 ★
Valid Bit of PTE is 1 ★Cached인 VM!
Page Fault : Reference to VM word that is not in physical memory

is equal to 'DRAM Cache Miss'
Valid Bit of PTE is 0 ★

Uncached인 VM!

Page Fault : Reference to VM word that is not in physical memoryis equal to 'DRAM Cache Miss'
Valid Bit of PTE is 0 ★
Valid Bit of PTE is 0 ★Uncached인 VM!Page Fault가 발생하면, 아래와 같은 처리 과정을 거친다.

Page Fault Exception이 발생한다.


Page Fault Handler가 Victim(Evict할 대상)을 일련의 Sophisticated Algorithm을 통해 선정한 후, Evict한다.

이때, Write-Back이 일어날 수 있다.



Page Fault를 일으켰던 'Offending Instruction'을 Restart한다.

이번엔 Page Hit이 될 것!





Page Fault Exception이 발생한다.


Page Fault Handler가 Victim(Evict할 대상)을 일련의 Sophisticated Algorithm을 통해 선정한 후, Evict한다.

이때, Write-Back이 일어날 수 있다.



Page Fault를 일으켰던 'Offending Instruction'을 Restart한다.

이번엔 Page Hit이 될 것!



Page Fault Exception이 발생한다.
Page Fault Handler가 Victim(Evict할 대상)을 일련의 Sophisticated Algorithm을 통해 선정한 후, Evict한다.

이때, Write-Back이 일어날 수 있다.

Page Fault Handler가 Victim(Evict할 대상)을 일련의 Sophisticated Algorithm을 통해 선정한 후, Evict한다.이때, Write-Back이 일어날 수 있다.
Page Fault를 일으켰던 'Offending Instruction'을 Restart한다.

이번엔 Page Hit이 될 것!

Page Fault를 일으켰던 'Offending Instruction'을 Restart한다.이번엔 Page Hit이 될 것!- 이러한 과정을 'Demand Paging'이라고 부르기도 한다. ★Uncached였던 PTE3(VP3)이 Cached가 되고, Cached였던 PTE4(VP4)가 Uncached가 된 것을 주목하자.PTE를 처음 할당할 때에는, Disk의 Virtual Memory와 연결된다. 즉, Uncached가 Default State인 것! ★★★Locality (Working Set)  이렇게 VM에 대해 간단히 알아보았는데, 얼핏 보았을 땐 상당히 비효율적인 과정처럼 보인다. 하지만, 실제로는 꽤나 준수한 성능을 보인다(그렇기에 현대에서도 적용되는 것). 이는 바로 'Locality(근접성)' 때문이다.
Working Set : 어떤 시점에 Process가 접근하고자 하는 'Active Virtual Page들의 Set을 의미한다.


만약, Working Set Size가 Main Memory Size보다 작으면, 'Compulsory Miss(초기 상태에서 메모리 Fetching 시 필연적으로 일어나는 Miss들)'를 제외하고는 Miss가 잘 일어나지 않아 프로그램 성능이 굉장히 우수하다. ★

프로그램이 'Temporal Locality(이전에 접근했던 메모리 영역을 재접근하는 근접성)'를 높이기 위해선 작은 사이즈의 Working Set을 가져야 한다. ★



만약, Working Set Size가 커져서, 그 합이 Main Memory Size보다 커지면, Page들이 계속해서 In & Out으로 Swap하기 때문에 성능이 굉장히 떨어지게 된다. ★

Page Fault가 굉장히 자주 발생하게 되는 것! (Disk I/O가 너무 많아지는 것)
이를 Thrashing 현상이라고 부른다. ★




Working Set : 어떤 시점에 Process가 접근하고자 하는 'Active Virtual Page들의 Set을 의미한다.
만약, Working Set Size가 Main Memory Size보다 작으면, 'Compulsory Miss(초기 상태에서 메모리 Fetching 시 필연적으로 일어나는 Miss들)'를 제외하고는 Miss가 잘 일어나지 않아 프로그램 성능이 굉장히 우수하다. ★

프로그램이 'Temporal Locality(이전에 접근했던 메모리 영역을 재접근하는 근접성)'를 높이기 위해선 작은 사이즈의 Working Set을 가져야 한다. ★

만약, Working Set Size가 Main Memory Size보다 작으면, 'Compulsory Miss(초기 상태에서 메모리 Fetching 시 필연적으로 일어나는 Miss들)'를 제외하고는 Miss가 잘 일어나지 않아 프로그램 성능이 굉장히 우수하다. ★프로그램이 'Temporal Locality(이전에 접근했던 메모리 영역을 재접근하는 근접성)'를 높이기 위해선 작은 사이즈의 Working Set을 가져야 한다. ★
만약, Working Set Size가 커져서, 그 합이 Main Memory Size보다 커지면, Page들이 계속해서 In & Out으로 Swap하기 때문에 성능이 굉장히 떨어지게 된다. ★

Page Fault가 굉장히 자주 발생하게 되는 것! (Disk I/O가 너무 많아지는 것)
이를 Thrashing 현상이라고 부른다. ★


만약, Working Set Size가 커져서, 그 합이 Main Memory Size보다 커지면, Page들이 계속해서 In & Out으로 Swap하기 때문에 성능이 굉장히 떨어지게 된다. ★Page Fault가 굉장히 자주 발생하게 되는 것! (Disk I/O가 너무 많아지는 것)
이를 Thrashing 현상이라고 부른다. ★
이를 Thrashing 현상이라고 부른다. ★~> CPU 성능도 중요하지만, 메모리 성능도 매우 중요하다. 메모리가 작으면, 많은 프로그램을 돌릴 경우, Thrashing이 발생해 속도가 저하되는 상황이 발생한다.  이외에도 VM에 관해선 상당히 깊고 다양한 개념이 존재한다. 그러나, 본 SP 연재에서는 여기까지만 다루도록 한다. 이보다 더 Low-Level한 Concepts는 Application 측면에서의 시스템 개념을 다루는 SP에서 소개하기엔 무리가 있기 때문이다. 자세한 내용은 추후 진행될 OS 연재에서 이어가도록 하겠다.   이렇게, System Programming 연재 대장정을 마친다. 첫 포스팅에서 언급했듯, 본인의 SP 개념 복습 목적으로 글을 쓰겠다 했는데, 이는 실제로도 학기 중에 꽤나 많은 도움이 되었다. 허나, 중요한 것은 이렇게 기록물을 남겨놓았기 때문에 앞으로도 계속해서 생각날 때마다 이 SP 연재를 다시 보며 기억을 떠올릴 수 있다는 것이다. 현재 System 쪽 진로를 희망하는 입장에서, 이는 굉장히 큰 무기가 될 것이라 생각한다. '어려운 SP 개념을 나만의 언어로 풀어가는 연습'을 많이 하게 되었고, 이는 추후 System을 더 깊게 공부하고, 연구하는 과정에서 분명 크나큰 도움이 되리라 믿는다.  한편, 분명 이 연재를 읽는 이가 언젠가라도 생길 것이라 믿는다. 포스팅 과정에서 정확치 않은 내용이 담겼을 수 있는데, 이를 너그러이 양해해주길 바란다. 또한, 본인의 평소 글쓰기 스타일이 워낙 주절주절하는 스타일이라 가독성이 조금 떨어졌을 수 있음도 인정한다. 이 역시 양해해주길 바란다. 그러나, 연재 과정에서 본인은 최대한 '타인에게 설명하는 기분으로', 그리고 '내가 교수가 된다면, 이렇게 설명하는게 좋겠다'는 마인드를 가지고 임했기 때문에, 분명 어느 정도 독자의 이해에 큰 도움이 되리라 자부한다. SP에 대한 개념 보충이 필요한 사람, 컴퓨터공학을 전공하지 않아 SP에 대한 개념 학습이 필요한 사람들이 읽으면 좋을 것이다. 본인이 SP를 공부하며 느꼈던 재미를, 다른이들도 느꼈으면 좋겠다.  현재 글을 작성하고 있는 시점이 2022년 6월 19일인데, 앞으로도 이 SP 연재 시리즈를 지속적으로 관리하며, 시각 자료를 보충 및 변경(참조 교재 자료를 최대한 배제하고, 최대한 내 스스로 표현한 그림 자료들로 대체할 것)하고, 개념을 보충하고 할 것이기 때문에, 앞으로도 이 SP 연재는 살아있을 것이다.  연재 중간에 언급한 것처럼, 사실 SP 연재는 여기서 끝은 아니다. 여기서 다룬 개념들을 토대로 실제 본인이 학부 SP 수업에서 수행했던 프로젝트들이 있는데, 이들을 적절히 변형(같은 강의 수강생의 Copy 방지)하고, Customizing해서 순차적으로 소개할 계획이다. 세 프로젝트는 각각 Linux Shell 제작, Concurrent Server 구축, Dynamic Memory Allcator 구현이다. 추후 이들에 대한 포스팅이 올라올 것이다.  아무튼, 2022년 상반기 System Programming 개념 설명은 공식적으로 여기까지로 한다. 하반기에 OS(Operating System) 연재로 다시 찾아올 것이다. System을 완벽히 이해하는 그 날까지!Stay Hungry, Stay Foolish!SP - 7.3 Library Interpositioning"
10,[OS] POSIX message passing,https://velog.velcdn.com/images%2Fchy0428%2Fpost%2F10fee1e7-bd48-438f-bf8c-ba3d68299bc3%2Fimage.png,"Operating system✅ msgget() - create a message queue데이터를 전송할 메시지 큐를 만든다.✅ msgsnd() - send a message to a message queue메시지 큐로 데이터를 전송한다. int msgsnd( int msqid, const void *msgp, size_t msgsz, int msgflg );msqid는 메시지 큐의 식별자이다.
msgp는 전송 대상인 데이터이다.

ㅡmsgsz: 위에 보듯 전송 데이터는 long 값을 첫번째에 가지고 있어, 원하는 데이터만 걸러낼 수 있다. 따라서, 데이터의 크기에 long 값이 반드시 들어가기 때문에 type을 나타내는 long크기는 제거한다.
msgflg의 값을 이용하여 메시지 큐에 있는 자료 전송 실패시의 동작을 선택할 수 있다. 0은 큐에 공간이 생길 때까지 기다리는 것이고, IPC_NOWAIT은 큐에 공간이 없을때 바로 -1로 복귀하는 것을 의미한다. ✅ msgrcv() - receive a message from a message queue메시지 큐로부터 데이터를 읽어온다.ssize_t msgrcv(int msqid, void *msgp, size_t msgsz, long msgtyp, int msgflg);
msqp 는 수신한 데이터이다.
msgtyp은 메시지 큐에 있는 데이터 중 어떤 데이터를 읽어 들일지에 대한 옵션으로, 0이면 큐의 첫번째 자료를 읽어 들이는 것이고, 양수일 경우, 양수로 지정한 값과 동일한 데이터 타입의 자료 중 첫번째를 읽어들인다. 음수일 경우, 음수 값을 절대 값으로 바꾼 후, 이 절대값과 같거나 가장 작은 데이터 타입의 데이터를 일어들인다.
msgflg는 읽어 들이는 옵션으로, IPC_NOWAIT일 경우, 메시지 큐에 메시지 없으면 기다리지 않고 -1로 복귀한다.✅ msgctl() – control/deallocate message queue메시지 큐의 현재 상태 정보를 알 수 있고, 변경하고 삭제할 수 있다.int msgctl ( int msqid, int cmd, struct msqid_ds *buf )msqid는 메시지 큐의 식별번호이다.
cmd의 종류에는 3가지가 있는데 IPC_RMID는 메시지 큐를 삭제하는 명령이다. 이 때는 buffer가 필요없으므로 0으로 지정한다. IPC_STAT는 현재 상태를 buf에 저장하는 것이고, IPC_SET은 현재 상태를 buf 값으로 변경하는 것이다. OS 3번째 과제에서 작성했던 코드가 해당 내용을 구현한 것이다. 시험 전에 코드 리뷰하기!
과제 내용과 코드는 공개하면 안되서 올리지 못한다! 호홍[OS] System-V shared Memory API[OS] 싱글스레드vs멀티스레드 "
11,Deadlocks,https://velog.velcdn.com/images/rlfrkdms1/post/c72492de-bb16-4fa4-b096-1e2ff2942cbe/image.png,"운영체제우리 말로는 교착상태라고도 한다. 
위의 그림을 보면 사거리에서 차들이 각자의 방향대로 진행하길 고집해 결국 모든 차가 움직이지 못하는 상태가 된 것을 알 수 있다. 특정 차들을 꺼내던지, 일련의 차들을 모두 뒤로 빼야하는데 이는 굉장이 오버헤드가 큰 작업이다. 누군가가 양보를 한다면, 쉽게 해결 할 수 있다. 그래서 컴퓨터 내에서도 희생을 하지 않고 자원을 가지고 잇으면서 내가 가지지 못한 자원은 요청하고, 또 상대방도 내가 요청한 자원을 가지고 있으면서 내가 가지고 있는 자원을 요청한다면 서로 원하는 자원을 영원히 갖지 못하는 상태에 빠지게 된다. 그래서 위의 그림에서 자신이 가진 자원과 요구하는 자원은 무엇인가? 가진 자원은 자신이 지금 가는 길목이고 요구하는 자원은 통과하길 원하는 길목일 것이다. 차가 여러대 있지만 같은 열에 있는 차들을 일련의 프로세스로 보자. 그러면 위와 같은 상황이 된다. 어느누구도 양보를 하지 않으면 더이상 진행되지 않는 상태이다. 일련의 프로세스들이 서로가 가진 자원을 기다리며 Block된 상태Resourse여기서 자원이라 함은 하드웨어 자원일 수도, 소프트웨어 자원일 수도 있다. 예시스템에 2개의 tape drive가 있고 프로세스 P1, P2가 있다고 가정하자
프로세스는 하나의 tape drive에서 자원을 읽어 다른 tape drive에 쓰는 작업을 한다. 따라서 하나의 프로세스가 두개의 tape drive를 점유해야한다.
이때 P1과 P2 각각이 하나의 tape drive를 보유한채 다른 하나를 기다리고 있다면 어느 누구도 진행이 안되는 상태에 빠진다. 이러한 경우는 하드웨어 자원의 경우이다. 이전에 process synchronization에서도 언급했던, semaphore에서도 deadlock이 발생할 수 있다.
P0이 S를 획득한 다음에 CPU를 빼앗겼고, P1은 Q를 획득한 다음 S를 얻으려 하는데 이미 P0가 가지고 있으니 P0가 내려 놓을 때 까지 획득하지 못한다. P0로 다시 CPU가 돌아가도 마찬가지이다.
보통 자원을 사용하는 절차는 4가지로 나누어진다.
1. 자원을 요청하는 단계
2. 자원을 획득하는 단계
3. 자원을 사용하는 단계
4. 자원을 반납하는 단계Deadlock 발생의 4가지 조건dead lock이 발생하기 위해선 아래의 4가지 조건을 모두 만족해야한다. Mutual exclusion매 순간 하나의 프로세스만이 자원을 사용할 수 있음우리 말로는 상호배제라고 한다. 자원을 일단 얻었으면 독점적으로 사용한다는 것이다.
다 사용했을 경우에 반환은 할 수 있다. No preemption프로세스는 자원을 스스로 내어놓을 뿐 강제로 빼앗기지 않음빼앗기지 않는다 것이다. 우리말로는 비선점이라고 한다. 자원을 가지고 있는데 가진 자원을 빼앗길 수 있다면 dead lock이 발생하지 않는다. Hold and wait자원을 가진 프로세스가 다른 자원을 기다릴 때 보유 자원을 놓지 않고 계속 가지고 있음내가 가진 자원은 내놓지 않으면서 추가적인 자원을 요청해 기다린다. Circular wait자원을 기다리는 프로세스간에 사이클이 형성되어야 함순환 대기라고 한다. 필요로 하는 자원들이 꼬리에 꼬리를 무는 것이다. 서로가 가진 자원을 기다리면서 사이클을 형성하는 경우이다. 위에서 보았던 사거리 그림에서도 사이클을 확인할 수 있다. 프로세스 P0,P1,...,Pn이 있을 때 P0은 P1의 자원을 기다리고, P1은 P2의 자원을, P2는 P3의 자원을 .. 결국엔 Pn이 P0의 자원을 기다리는 형태이다. 조건들이 구분이 잘 되지 않을 수 있으나. 빼앗기는 것과 내어놓는 것을 구분해야한다. 빼앗는 것은 비자발적이고, 내어놓는 것은 자발적인 것이다. Deadlock이 발생했는지를 알아보기 위해 자원할당 그래프를 그려본다. 그림의 동그라미는 프로세스를 나타내며, 사각형은 자원을 나타낸다. 화살표는 두가지 종류가 있다. 
자원 -> 프로세스
- 해당 자원이 지금 프로세스에 속해있음을 의미한다. 
자원 -> 프로세스
- 해당 자원이 지금 프로세스에 속해있음을 의미한다. 
프로세스 -> 자원
- 프로세스가 자원을 요청하는 것이다.
- 요청했지만, 아직 획득하진 못한 단계이다.
사각형안의 점들은 자원의 인스턴스(수)를 얘기하는 것이다. 따라서 사각형내에 점이 두개라면 해당 자원은 인스턴스를 두개 가진 것이다. 
프로세스 -> 자원
- 프로세스가 자원을 요청하는 것이다.
- 요청했지만, 아직 획득하진 못한 단계이다.
사각형안의 점들은 자원의 인스턴스(수)를 얘기하는 것이다. 따라서 사각형내에 점이 두개라면 해당 자원은 인스턴스를 두개 가진 것이다. 
P1 : R2자원 보유, R1 자원 요청
P1 : R2자원 보유, R1 자원 요청
P2 : R1,R2 자원 보유, R3 자원 요청
P2 : R1,R2 자원 보유, R3 자원 요청
P3 : R3 자원 보유
P3 : R3 자원 보유화살표를 따라가보면서 사이클이 있는지 확인하면 위의 그림에선 Deadlock이 발생하지 않는 것을 볼 수 있다. 
하지만 이 그림을 살펴보자.T1 → R1 → T2 → R3 → T3 → R2 → T1 T2 → R3 → T3 → R2 → T2사이클이 두개 있는 것을 확인할 수 있다.
하지만 사이클이 있다고해서 모두 Dead lock에 걸리는 것일까? 아니다. 자원당 하나의 인스턴스를 가지고 있다면 dead lock이 맞다. 하지만 자원하나에 여러 인스턴스가 존재한다면 무조건 Dead lock은 아니다. 따라서 다른 조건도 따져봐야한다. R2 자원의 인스턴스가 두개이지만 둘 다 이미 프로세스에 각각 할당되어 있고, 할당된 프로세스들은 다른 자원을 추가로 요청하고 있으므로 절대 내어놓지 않을 것이다. 따라서 dead lock 상황이다.
위 그림에서도 T1 → R1 → T3 → R2 → T1 로 사이클이 존재한다.
하지만 사이클과 관련된 자원 R1, R2에 인스턴스가 각각 2개이다. 각 인스턴스는 프로세스가 가지고 있지만, 가지고 있는 프로세스인 P2, P4가 다른 자원을 요청하지 않기 때문에 쓰고 나서 반납할 것이다. 따라서 반납하고 나면 자원이 여유로워지므로 다른 프로세스가 사용할 것이다. 따라서 dead lock 이 아니다. dead lock을 처리하는 방법Deadlock Prevention, Deadlock Avoidance는 Deadlock이 생기기 이전에 미연에 방지하는 것이다.
Deadlock Detection and recovery, Deadlock Ignorance는 Deadlock이 생기도록 놔두고
위로 갈수록 더 강한 방법이다.Deadlock Prevention자원 할당 시 Deadlock의 4가지 필요 조건 중 어느 하나가 만족되지 않도록 하는 것네가지 조건 중 하나를 원천적으로 차단해 Deadlock에 들어가지 못하게 하는 것이다.
Deadlock을 원천적으로 막을 순 있지만, 자원에 대한 이용률이 낮아지고, 전제 시스템의 성능이 나빠지고 starvation 문제가 생길 수 있다.
생기지도 않은 Deadlock을 우려해 제약조건을 많이 걸어놓기 때문에 비효율적이다. 공유해서는 안되는 자원의 경우 반드시 성립해야 한다.이 조건은 막을 수 있는 조건은 아니다. 한번에 여러 프로세스를 공유할 수 있는 자원이라면 애초에 Deadlock이라는 이야기가 나오지도 않았을 것이다. 그래서 이 조건은 한번에 하나의 프로세스만 사용할 수 있는 자원이라면 우리가 배제할 수 있는 조건은 아니다. 가지고 있는 자원을 뺏어올 수 없기 때문에 Deadlock이 생긴다. 따라서 자원을 빼앗을 수 있게 하면 된다.
CPU는 preemptive 스케줄을 사용했다. CPU를 받고 나면 영원히 반납하지 않을 수 있기 때문에 timer를 둬 특정 프로세스가 CPU를 독점하지 못하도록 했다. 그래서 CPU는 항상 빼앗아올 수 있었다. 이러한 자원은 Deadlock이 걸리지 않는다. 자원을 아무렇게나 빼앗아 오면 문제가 생길 수 있다. 하지만 CPU나 memory는 어떻게 쉽게 빼앗아 오는 것일까? 자원의 현재 사용상태를 save하고 restore할 수 있기 때문이다. 어떤 자원은 중간에 빼앗아오면 하던일에 문제가 생길 수 있다. 그래서 이러한 자원들은 preemption을 사용하기 어렵다. 프로세스가 자원을 요청할 때 다른 어던 자원도 가지고 있지 않아야 한다. 내가 가진 자원은 내놓지 않으면서 추가적인 자원을 요청해 기다리기 때문에 Deadlock생긴다.
자원을 기다려야하는 상황에서는 자원을 보유하고 있지 않으면 되는 것이다.
그래서 이를 해결하는 방법은 두가지가 있다. 프로세스가 시작될 때 프로세스가 평생에 필요로 하는 자원을 모두 할당받게 한다. 
종료 될 때 모든 자원을 반납하고, 중간에 추가적인 자원을 필요로 하지 않으므로 Deadlock이 생기지 않음
하지만 매 시점마다 필요로 하는 자원이 다르기 때문에 특정 자원이 필요하지 않은 상황에서도 해당 자원을 가지고 있게 되면 비효율적인 자원 소모가 이루어진다. 
종료 될 때 모든 자원을 반납하고, 중간에 추가적인 자원을 필요로 하지 않으므로 Deadlock이 생기지 않음하지만 매 시점마다 필요로 하는 자원이 다르기 때문에 특정 자원이 필요하지 않은 상황에서도 해당 자원을 가지고 있게 되면 비효율적인 자원 소모가 이루어진다. 필요할 때 자원을 그때그때 할당받는다. 하지만 자원을 요청할 때 가지고 있는 자원을 모두 내려놓고 대기한다. 
자진해서 반납
그러면 대기하고 있던 누군가가 자원을 사용할 수 있다. 
자진해서 반납그러면 대기하고 있던 누군가가 자원을 사용할 수 있다. 자원을 기다리는 프로세스간에 사이클이 형성되어야 함필요한 자원들이 꼬리에 꼬리를 물고 있으면서 사이클이 형성된 경우이다.
이걸 막기 위해선 자원마다 순서를 매기는 것이다.
내가 1,3,5번 자원이 필요할 때 숫자가 낮은 것 부터 획득할 수 있게 한다. 그러면 1번 자원을 획득해야 3번 자원을 획득할 수 있게 하면 Deadlock이 생기지 않는다.
왜냐하면 누군가는 1번 자원을 가지고 있으면서 3번 자원을 기다리고 누군가는 3번 자원을 가지고 있으면서 1번 자원을 기다려야 Deadlock인데, 1번과 3번 모두가 필요하면 모두가 1번 부터 획득해야하기 때문이다.
따라서 사이클이 생길 우려가 없다. Deadlock Avoidance자원 요청에 대한 부가적인 정보를 이용해 Deadlock의 가능성이 없는 경우에만 자원을 할당
시스템 state가 원래 state로 돌아올 수 있는 경우에만 자원 할당prevention과 마찬가지로 미연에 Deadlock을 방지하는 방법이다.
자원을 요청하는 것에 대해 부가적인 정보를 이용해 자원이 있지만 Deadlock일 가능성이 전혀 없는 경우에만 자원을 할당
보통은 어떻게 동작하냐 ?
프로세스가 시작해서 종료될 때 까지 사용하려는 자원의 수나, 종류가 다 다르다. 그렇기 때문에 Deadlock avoidance는 프로세스가 시작될 때 프로세스가 평생 쓸 자원의 최대량을 알고 있다고 가정하고 Deadlock을 피해간다. 평생 쓸 자원을 알고 있기 때문에 자원을 할당해줬을 때 Deadlock이 생길 수 있다고 판단되면 자원을 할당해주지 않는 것이다. 두가지로 나눠서 생각할 수 있다. 자원의 인스턴스가 하나씩밖에 없을 때는 그래프를 통해 Avoidance를 할 수 있다. 자원의 인스턴스가 자원당 여러개가 있다면 Banker's Algorithm을 사용할 수 있다. 자원이 두개 있다. 점선 화살표는 프로세스 -> 자원, 이 프로세스가 평생에 적어도 한번은 이 자원을 한번은 사용할 일이 있음을 의미한다. Deadlock Avoidance는 프로세스가 시작될 때 프로세스가 평생에 사용할 자원들을 미리 다 declare를 하고 그걸 이용해 Deadlock을 피해갈 수 있는 방법을 사용한다고 했었다. 그래서 점선 화살표를 이용해 지금은 사용하지 않지만 미래에 사용할 자원을 표시해놓는 것이다. 첫번째 상황에서 P2가 R2를 요청하면 두번째 그림과 같이 된다. 이때 R2는 아무도 가지고 있지 않기 때문에 P2가 가지고 있게 된다. 따라서 마지막 그림과 같은 상황이 된다. 최종적으로 마지막 상황은 Deadlock이 아니다. 왜냐하면 P1이 R2를 미래에 요청할 수 있지만, 지금은 요청하지 않은 상태이기 때문에 사이클이 형성되지 않는다. 실제 요청을 하는 시점에 점선이 실선으로 바뀌며 Deadlock에 걸린다. 마지막 그림은 Deadlock이 아니지만 (P1이 R1을 사용하고 반납하면 P2가 R1을 사용하면 되기 때문이다.) Deadlock Avoidance는 최악의 상황을 생각한다. 그래서 두번째 그림의 상황에서 R2를 P2에게 할당하게 되면 세번째 그림이 된다. 따라서 Deadlock의 가능성이 있는 상황이 되므로 R2를 할당하지 않는 것이다. 그래서 R2를 아무도 가지고 있지 않은데도 아무도 가지지 못하는 상황이 생긴다. 이후에 P1이 R2를 요청하면, 사이클이 생길 가능성이 전혀 없기 때문에 R2를 할당해준다. P1이 R2를 다 사용하고 반납하면 그 이후에는 P2가 R2를 할당받아도 사이클이 생기지 않기 때문에 P2가 R2를 할당 받는다. 5개 프로세스 -> P0,P1,P2,P3,P4
3개 자원 -> A,B,C 는 각각 10,5,7개의 인스턴스를 가지고 있다. Deadlock Avoidance는 평생 사용할 자원의 최대량을 알고 있다고 가정한다고 했다. 이때 이 최대량이 Max이다.
Allocation : 할당 받은 자원
Max에서 Allocation을 빼면 Need가 되는 것이다.
Available : 추가로 할당할 수 있는 양(남은 가용 자원)
자원을 줄 수 있지만 문제가 생길 가능성이 있으면 주지 않는 방법이다. 원래 (10,5,7)의 인스턴스가 있지만 이미 할당된것 (0+2+3+2+0, 1+0+0+1+0, 0+0+2+1+2)를 빼면 가용자원은 (3,3,2)가 남는다. 이때 각 프로세스에 자원을 할당할 수 있는지 확인해보자.P0
P0이 필요한 데이터는 Max(7,5,3)에서 Allocation(0,1,0)을 뺀 Need(7,4,3)이다.
하지만 현재 가용 자원은 (3,3,2)이므로 (7,4,3)을 할당할 수 없다. 따라서 P0에는 자원을 할당해주지 않고 넘어간다. (요청을 받아들이지 않는다.)
Max를 지금 당장 요청하지 않고 조금만 요청해 가용자원인 (3,3,2)를 줄 수도 있지만, P0가 Max를 모두 요청해서 이를 받아들이게 되면 문제가 생길 수 있다. 당장은 deadlock이 생기지 않는다. 왜냐하면 P0가 데이터를 대기하는 동안 다른 프로세스가 사용하던 자원을 반납할 수 있기 때문이다. 하지만 Banker's Algorithm은 굉장히 보수적이다. 따라서 본인의 최대 요청을 할 것이라고 가정을 한다.
가용자원으로 처리 가능 -> 요청 받아들임
가용자원으로 처리 불가능 -> 요청 무시
P0가 지금 당장은 Max가 아닌 A를 하나만 달라고 할수도 있지만, Max를 달라고 할 위험성이 있기 때문에 Max를 모두 처리하지 못한다면 요청을 받아들이지 않는다. P1
P1의 need가 (1,2,2)이므로 available인 (3,3,2)가 모두 커버 가능하다. 필요한 만큼 할당이 되었다면, 다시 반납할 것이므로 이제 available은 Allocation(2,0,0)을 더한 것이 된다. 즉, available = (5,3,2)이다.
왜 요청을 받아들이는가? 최대 요청보다 더 요청할 일은 없는데, 최대 요청을 모두 커버할 수 있기 때문이다. 따라서 언젠가는 반환할 일만 남았다. P2
P2의 Need가 (6,0,0)이므로 가용자원인 (5,3,2)이 커버하지 못한다. 그러므로 요청을 받아들이지 않는다. P3
Need가 (0,1,1)이므로 가용자원 (5,3,2)으로 커버할 수 있다. 따라서 이제 가용자원은 (7,4,3)이 된다. P4
Need가 (4,3,1)이므로 가용자원 (7,4,3)으로 커버할 수 있다. 따라서 이제 가용자원은 (7,4,5)이다. P0
Need가 (7,4,3)이므로 가용자원 (11,7,4)로 커버할 수 있다. 따라서 이제 가용자원은 (7,5,5)이다. P2
Need가 (6,0,0)이므로 가용자원 (7,5,5)로 커버할 수 있다. 따라서 이제 가용자원은 원래 인스턴스인 (10,5,7)이다. 따라서 sequence<P1,P3,P4,P2,P0>가 존재한다. 이러면 deadlock은 생기지 않으나, 굉장히 비효율적이다. 왜냐면 요청을 안할지도 모르는데 혹시나 최대 요청을 할 것을 대비해 자원이 있는데도 요청을 받아들이지 않기 때문이다. 특정 프로세스의 요청을 가용자원으로 충족하지 못한다고 하더라도 다른 프로세스들이 반납할 수도 있으므로 알 수 없는 것인데 이 알고리즘에서는 항상 최악의 경우를 생각한다. 
항상 safe한 상태만 생각한다.
가용자원이 max 자원을 만족시켰기 때문에 해당 프로세스는 언젠가 종료될 것이다. 종료되면 가진 자원을 다 내려놓는다. 그렇게 되면 가졌던 자원도 모두 가용자원이 된다. 그래서 이 가용자원을 가지고 최대 요청을 충족할 수 있는 프로세스에게 주는 것이다. 따라서 프로세스 n개를 현재의 가용자원만으로 처리될 수 있고, 그 뒤의 프로세스는 뱉어낸 자원과 가용 자원으로 처리될 수 있다. 이런식으로 끝까지 sequence가 한가지 존재한다면 안전한 상태인 것이다. 이러한 상태를 safe 상태라고 한다. 그래서 모든 프로세스가 끝까지 수행되는 것을 보장하는 상태이다. P0가 A를 하나 요청하면 A가 남아있으니까 줄 수 있다. 문제가 되느냐 ? 문제가 되는 건 없다. safe한 상태에서 가용자원만으로 충족되지 않는 프로세스에게 자원을 준다고 해서 deadlock인 것은 아니다. 자신이 가지고 있는 것을 내려 놓지 않고 요청하고 대기만 하면 deadlock이다.
그래서 굉장히 안전하게 가자는 알고리즘이다. 결론적으로 미리 deadlock을 막는 방법이다. Deadlock Detection and recoveryDeadlock 발생은 허용하되 그에 대한 detection 루틴을 두어 Deadlock 발견시 recovery시스템이 느려지거나 할 때 혹시나 Deadlock이 생긴 것은 아닌가 detection을 하고 있으면 recovery하는 것이다. 자원당 인스턴스가 하나뿐인 경우에는 자원할당 그래프를 이용해 detection 한다.
자원 할당 그래프를 오른쪽 그래프와 같이 자원은 빼버리고 프로세스끼리만 연결해 간단하게 그릴 수 있다. 왜냐하면 사이클이 나오는 상황은 둘 다 똑같기 때문이다.
그렇다면 오른쪽 그래프에서 사이클을 찾는 오버헤드?
프로세스가 n개가 있다면 O(n^2)시간이 걸린다. 왜냐하면 프로세스가 n개 있기때문에 다 화살표를 따라가보면 된다. 프로세스가 n개이면 화살표는 최대 n(n-1)개가 있다. 모든 점을 다 탐색해보는 것이기 때문에 DFS,BFS와 같다고 할 수 있다.
자원당 인스턴스가 여러개일 경우에는 아래와 같이 sequence를 확인해 detection할 수 있다.

여기선 Max에 대해 고려하지 않고 현재의 요청인 Request에 대해서만 고려한다.
위의 sequence를 확인해보면 <P0,P2,P3,P1,P4>가 존재한다. 만약 P2가 자원 C를 하나 더 요청하면 어떻게 될까?
dead lock이 걸린다 ! 왜냐하면 P0를 커버하고 나서 모든 프로세스에 대해 커버하지 못하기 때문이다. 그래프를 이용하는 것이 위의 sequence를 확인하는 것의 subset이다. 그래서 인스턴스가 하나여도 sequence를 찾는게 더 간단하다. 
Process termination

Deadlock에 연루된 프로세스들을 사살 -> 한꺼번에 죽인다. 
Deadlock에 연루된 프로세스들을 하나씩 죽여보는 것이다. -> 하나씩 죽여보고 Deadlock인지 확인하고 Deadlock이 아직도 걸려있다면 또 죽인다.

Process terminationDeadlock에 연루된 프로세스들을 사살 -> 한꺼번에 죽인다. Deadlock에 연루된 프로세스들을 하나씩 죽여보는 것이다. -> 하나씩 죽여보고 Deadlock인지 확인하고 Deadlock이 아직도 걸려있다면 또 죽인다.
Resource Preemption

비용을 최소화할 희생양을 하나 찾아 자원을 뺏는다. 
safe state로 rollback해 프로세스를 restart하면 Deadlock이 없어진다. 
문제는 Deadlock을 없애놨더니 똑같은 패턴이 또 나타날 수 있는 것이다. 즉, 계속 동일한 프로세스가 희생양으로 선정되어 starvation 문제가 생길 수 있다. 그래서 Deadlock이 발생했을 때 수행하는 패턴을 계속 조금씩 바꿔야한다. 따라서 자원을 몇번 뺏겼는지 rollback 횟수도 cost factor에 고려해 자원을 뺏는 방법에 대해 고민해야 한다. 

Resource Preemption비용을 최소화할 희생양을 하나 찾아 자원을 뺏는다. safe state로 rollback해 프로세스를 restart하면 Deadlock이 없어진다. 문제는 Deadlock을 없애놨더니 똑같은 패턴이 또 나타날 수 있는 것이다. 즉, 계속 동일한 프로세스가 희생양으로 선정되어 starvation 문제가 생길 수 있다. 그래서 Deadlock이 발생했을 때 수행하는 패턴을 계속 조금씩 바꿔야한다. 따라서 자원을 몇번 뺏겼는지 rollback 횟수도 cost factor에 고려해 자원을 뺏는 방법에 대해 고민해야 한다. Deadlock IgnoranceDeadlock 을 시스템이 책임지지 않음Deadlock에 대해 아무것도 하지 않는다. 현대의 운영체제들은 이 방법을 채택하고 있다. 그럼 어떻게 해결하는가? 운영체제가 관여하지 않으면, 사람이 프로세스를 돌리다가 돌아가지 않으면 사람이 해결한다. Deadlock은 빈번히 발생하는 이벤트가 아니기 때문에, 이를 미연에 방지하기 위해 훨씬 많은 오버헤드를 들이는 것이 현대적인 시스템에서는 비효율적이기 때문에 이를 처리하지 않는다. Deadlock을 방지, detection을 하는 것이 시스템의 오버헤드를 발생시키기 때문에 Deadlock이 생겨도 그냥 두는 것이다. 따라서 Deadlock이 생기면 시스템이 느려지거나 멈출 것이다. 이땐 사용자가 이를 감지하고 프로세스를 멈추던지 해결을 해야한다. 현대의 운영체제들이 이방법을 채택하고 있다. Operating System Concepts 10th
KOCW 강의 - [운영체제] 이화여자대학교 반효경 교수Process Synchronization (Concurrency Control)"
12,[OS] Mutex and Semaphore,https://velog.velcdn.com/images/chy0428/post/7e7bc0e3-46a8-4c74-9e53-35142037ee39/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-05-07%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.48.03.png,"Operating system지비씨 리눅스 과제할때 급하게 주워담은 지식으로 구현하던 뮤텍스와 세마포어를 드디어 배웠다 둑흔
일단 구현말고 전체적인 개념을 이해하기엔 여기 블로그에 참 좋은 예시로 쉽게 설명이 되어있었다. 보고 흐름을 잡으면 좋을 것 같다.Mutex뮤텍스를 이용할 때는 available이라는 변수를 사용할 것인데, 이는 이전에 살펴보았던 lock과는 작동 원리가 반대이다. lock은 false일 때, 임계구역에 들어갈 수 있었는데 available은 true일 때 임계구역에 들어갈 수 있다.
뮤텍스와 관련된 유명한 화장실 예시를 생각해보면 된다. 뮤텍스는 화장실이 1개뿐인 식당과 유사하다. 화장실을 가기 위해 카운터에서 키를 받아가는데, 누군가 이미 화장실에 갔다면 키가 없을 것이고 (available = 0), 이용 가능하다면 키가 있을 것이다. (available = 1) 즉, 뮤텍스는 키에 해당하는 어떤 object가 있으며 이 object를 소유한 프로세스만이 공유 자원에 접근할 수 있다
Semaphore세마포어를 integer 변수 S라고 할 때, 이 변수는 오직 wait()와 signal()이라는 2가지 atomic operation에 의해서만 엑세스 가능하다.

wait()연산은 공유 자원을 획득하기 위한 진입 영역, signal()연산은 공유 자원을 반납하는 영역에 해당한다. 이 연산들은 반드시 독립적이고 원자적으로 수행되어야 한다. block & wakeup
세마포어는 여전히 busy-waiting 문제를 가지고 있다. 예를 들어 이진 세마포어의 경우 S가 0일 때 어떤 프로세스는 wait() 연산 내부의 while 문을 돌면서 다른 프로세스가 signal() 연산을 해줄 때까지 기다리며 CPU time을 소모하게 된다.spinlock이라고도 불리는 busy waiting 문제를 block & wakeup 방식의 세마포어 구현을 통해 해결할 수 있다. 세마포어는 다음과 같은 구조체로 정의된다.
value는 이용 가능한 리소스의 개수, list는 waiting queue를 의미한다.Wait와 Signal 연산 같은 경우 다음과 같다.
wait 연산이 호출되고 가용 자원이 남아있지 않다면 이 프로세스를 waiting queue에 추가하고 프로세스의 상태를 대기 상태로 전환시킨다. 이후 control이 CPU 스케줄러에게 넘어가고 스케줄러는 다른 프로세스를 선택하여 실행시킨다.으악 너무 바쁘다 디비 프로젝트 바보 (20.05.06 추가예정이지만 언제 추가할 지 모르겠다 ㅜㅜ 종강 주세여)[OS] H/W support for Synchronization"
13,[OS] H/W support for Synchronization,https://velog.velcdn.com/images/chy0428/post/b5ee23d4-750f-45f3-99c1-2f5c8b37b8c0/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202020-05-05%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2011.41.26.png,"Operating system✅ Memory Model메모리 모델은 컴퓨터 아키텍처가 응용 프로그램에 제공 할 메모리를 결정하는 방법이다.a memory modification on one processor is immediately visible to all other processors.퍼포먼스 측면에서 손해가 있을 수 있다.modifications to memory on one processor may not be immediately visible to other processorscache가 다시 write-back될 때 반영되는 것처럼 즉각 반영되지 않는다. 퍼포먼스를 위해 이것을 사용한다. ✅ Memory Barriers (== memory fences)memory barriers는 cpu나 컴파일러에게 특정 연산의 순서를 강제하도록 하는 기능이다.
(All loads and stores are completed before any subsequent load or store operations are performed)
지난 포스팅에서 피터슨 알고리즘의 문제점과 마찬가지로 Thread2에서 x = 100과 flag = true의 실행 순서가 바뀔 수 있다. 두 개의 코드 사이에 memory barrier()를 집어넣는다. memory barrier는 barrier 앞에 존재하는 메모리 읽기/쓰기 연산들이 실제 메모리에 커밋되는 것을 보장한다. 위의 예제에서 Thread1의 while(!flag) 조건이 깨지는 순간에는 x에 100이 들어가 있어, print x를 실행하였을 때, 100이라는 수를 출력하게 된다.✅ Hardware Instructions
H/W support makes it easier and improve efficiency.✔️ Mutual Exclusion by Lock variablebollean lock = 0;초기값이 0이면, 아무도 안 들어간 것이다. 임계영역에 들어간 다음 lock을 true로 만든다. (== 잠근다.)
다른 프로세스들은 entry section에서 기다려야 한다. 임계영역에 들어갔던 프로세스가 나올 때는 lock을 false로 만들어 lock을 풀고 다른 프로세스가 임계영역에 접근 할 수 있도록 한다.)
critical section에 들어가면 true(1), 들어가지 않으면 false(0)
만약 위의 P0, P1 2개의 프로세스가 동시에 실행 될 경우, 둘 다 critical section에 들어가게 된다. 따라서, mutual exclusion을 보장할 수 못한다.
❓왜 이런 일이 발생하는가? while(lock);과 lock = true;가 한덩어리로 실행된다는 것을 보장할 수 없기 때문이다. P0에서 while이 실행된 후, lock = true;가 실행 되기 전에 P1에서 while(lock);, lock = true;을 실행하여 critical section에 들어가버리면 결국 2개의 프로세스 모두 critical section에 들어가는 것을 막을 수 없다.
💡solved key idea: 두 코드를 한 번에 실행시켜야 한다.
❗️이 경우, 위의 사진에 표시한 것과 마찬가지로 while(lock);, lock = true; 대신 while(TestAndSet(&lock));를 쓰면 된다. 자세한 내용은 뒤에서 알아보겠다.✅ Interrupt DisableInterrupt Disable는 하드웨어에 의한 mutual esclusion을 보장하는 방법이다.
이 방법은 interrupt를 disabling 하는 것이다. 인터럽트를 disable하면, context switching이 발생하지 않아 race condition이 생기지 않는다.
하지만 멀티프로세서 프로그램과 잘맞지 않고, 비효율적이라 잘 사용하지 않는다.✅ Hardware instructions하드웨어에서 다음과 같은 atomic instructions을 제공하면, locking을 쉽게 처리할 수 있다.파라미터로 전달된 target은 true값을 갖게 되고, return 값은 target이 갖고 있던 값이 된다.두 개의 파라미터 값을 exchange한다.val와 expected 값이 같을 경우, new_val의 값을 val에 넣고 val의 초기값을 리턴한다.
만약, val와 expected 값이 같지 않을 경우, 기존 val의 값을 유지한 채 val 값을 리턴한다.✅ Mutual Exclusion using TestAndSet공유변수는 lock이며 초기값은 false(== 0)이다.
앞에서 설명한것과 같이 TestAndSet에 초기값 0을 가지고 있는 lock을 파라미터로 전달하면 0을 return한다.
따라서 while문의 조건이 fail되므로 기다리지 않고 critical section으로 들어간다. 이 순간 TestAndSet의 정의에 의해 lock variable은 true값을 가지게 된다. (리턴 값은 0, lock은 1)
두번째 프로세스가 들어오면 lock은 true이므로, while문 조건에 걸려 기다려야 한다.
첫번째 프로세스가 exit section에서 lock = false;를 실행시키면, while문에서 TestAndSet의 return value가 false가 되어, 두번째 프로세스가 critical section에 들어갈 수 있다. ✅ Mutual Exclusion using Swap공유변수는 lock이며 초기값은 false이다. key라는 local variable에 true를 assign한다.
초기에는 Key에 true를 넣었기 때문에 while문이 true가 되고, swap을 실행시킨다.
key값은 true였으므로 lock에는 true가 들어가고, key에는 원래 lock에 있던 값인 false가 들어가게 된다. 다시 while loop 조건에서 key는 false이기 때문에, 조건이 fail되면서, 첫번째 프로세스는 critical section으로 들어갈 수 있게 된다.
두번째 프로세스가 entry section에 진입했을 때 lock에 true가 있는 상태에서, swap을 실행하면 여전히 key값이 true이므로, while-swap을 계속 실행하면서 첫번째 프로세스가 exit section에서 lock을 false로 만들기를 기다리게 된다.✅ Mutual Exclusion using CASlock은 false로 초기화되어 있다. compare_and_swap의 정의에 따라 리턴값은 lock에 있는 초기값인 false이므로 while문의 조건이 깨지게 되고 첫번째 프로세스는 critical section에 들어가게 된다. 그 순간 lock 변수는lock = lock == false ? 0 : 1  조건에 의해 1의 값을 가지게 된다.
두번째 프로세스가 entry section에 들어왔을 때에는 lock 변수가 1의 값을 가지고 있기 때문에 1을 리턴해주면서 while문에서 기다리게 된다. 첫번째 프로세스가 임계영역 코드 실행을 마치고 exit section에 가서 lock을 0으로 바꿔주면, 두번째 프로세스가 조건을 탈출하여 critical section에 들어올 수 있게 된다. ✔️ Bounded Waiting Mutual Exclusion
shared resource에 접근할 수 있는 기회를 랜덤하게 주는 것이 아니라 오른쪽에 있는 프로세스에게 다음 들어갈 기회를 준다. 위의 그림과 같은 경우, 공유자원을 스고 싶어하는 프로세스는 P0, P1, P3, P5이다. 현재 P1이 공유 자원을 사용하고 있고, 그 다음에는 P3이 들어갈 수 있도록 하는 것을 의미한다. 구현한 알고리즘을 살펴보자...shared variablesmutual exclusion을 위한 lock과 프로세스가 critical section에 들어가고 싶은지를 나타내는 waiting[n] 변수가 필요하다. (n은 프로세스의 갯수를 나타낸다.)
i번째 waiting이 true라는 것은, Pi가 critical section에 들어가길 원하지만 아직 들어가지 않았다는 것을 의미한다. 실제로 들어가 있는 프로세스와 들어갈 마음이 없는 프로세스의 waiting 변수는 false값을 가진다.Algorithm using TestAndSetTestAndSet을 이용한 알고리즘은 다음과 같다.
주석처리한 부분을 잘 읽어보면 이해가 될 것이다! 내가 좋아하는 검정ver

Algorithm using CAS
알고리즘은 위와 같고, CAS가 동작하는 방식을 이해하면 위와 유사하기 때문에 이해할 수 있을 것이다.Atomic Variable참똑똑해 저런방법을 사용하려 하다니..[OS] critical section problem & peterson's algorithm[OS] Mutex and Semaphore"
14,[OS] System-V shared Memory API,https://velog.velcdn.com/images%2Fchy0428%2Fpost%2F9ffa9df7-64b4-408e-8dd6-3cfdffd6a633%2Fimage.png,"Operating system이전 포스팅에서 설명한 IPC의 방법 중 하나인 Shared-Memory를 구현하는 방법에 대해 알아보겠습니다!System V✅ shmget() - create shared memoryint shmget(key_t key, int size, int shmflg);key : Key of shared memory segmentkey는 ftok()함수를 통해서 생성된 값이나, 또는 임의의 숫자를 사용한다.IPC_PRIVATE : key를 IPC_PRIVATE로 설정하면, key값은 중복되지 않는 임의의 값으로 자동으로 생성된다. size : size of shared memory segment할당할 메모리의 byte단위 크기주로 Buffer size가 들어간다.shmflg : flagsshared memory에 대한 설정값으로 IPC_CREAT, IPC_EXCL, access권한 9 bit의 bit or 연산으로 설정한다.S_IRUSR : 사용자 읽기 (사용자가 파일과 디렉토리 항목들을 읽을 수 있다.S_IWUSR : 사용자 쓰기 (사용자가 파일을 기록, 제거, 생성할 수 있다.) IPC_CREAT : shared memory를 생성한다.IPC_EXCL : key로 생성된 shared memory segment가 없는 경우에만 생성하고, 이미 같은 key로 shared memory가 생성되어 있다면 오류가 발생한다.❓ 둘다 포함되어 있지 않은 경우 ❓ 이미 만들어진 shared memeory에 대한 shm id를 return한다.✅ shmat() - attach shared memory to address space of processvoid shmat(int shmid, char *shmaddr, int shmflg);shmidshmget() 리턴으로 얻은 세그먼트 IDshmaddrattach 되도록 기대되는 메모리 주소 (보통 NULL)shmflgattach flags공유메모리는 attach된 주소부터 쓰여지게 된다.✅ shmdt() - detach shared memory from address space of processvoid shmdt(char *shmaddr);✅ shmctl() - deallocating a shared memory blockshmctl(shmid, IPC_RMID, NULL);shm_attach가 0이 되었을 때, shared memory block을 deallocate한다.POSIX✅ shm_open() - process creates or open shared memory segmentshm_id = shm_open(name, O_CREAT | O_RDWR, 0666);O_CREATcreat if it does not yet existO_RDWRopen for reading and writing✅ ftuncate() - set the size of the objectftuncate(shm_id, 4096);✅ mmap() - map shared memory segment to process address spacemmap(0, 4096, PROT_WRITE, MAP_SHARE, shm_id, 0)메모리를 매핑은 파일을 프로세스의 메모리에 매핑하는 것이다.
위의 예제는 매핑된 파일 읽기/쓰기 모두 가능하다.MAP_SHARED: 다른 유저와 데이터의 변경 내용을 공유한다.
PROT_READ: 매핑된 파일을 읽기만 한다.
PROT_WRITE: 매핑된 파일에 쓰기를 허용한다.
PROT_EXEC: 매핑된 파일을 실행할 수 있다.
PROT_NONE: 매핑된 파일에 접근할 수 없다.매핑이 성공하면, mmap은 공유메모리의 포인터를 리턴한다. ✅munmap(), shm_unlink()공유메모리 사용을 마쳤으면 위의 2개의 함수로 unmapping과 unlink를 하여야 한다.
만약, 수행하지 않는다면 프로세스는 종료되어도 메모리 영역에 공유메모리가 제거되지 않아 비정상적인 작동으로 이어질 수 있다.
[OS] 멀티프로세싱, 멀티프로그래밍, 멀티태스킹, 멀티스레딩[OS] POSIX message passing"
15,"[OS] 프로세서, 프로세스, 프로그램, 스레드",https://velog.velcdn.com/images/chy0428/profile/75ae30df-2624-4c15-8ae4-68375ddebffb/image.jpg,"Operating system용어정리✔️ 프로세서 (Processor)컴퓨터 운영을 위해 기본적인 명령어들을 처리하고 반응하기 위한 논리회로이다. CPU를 뜻하며, PC나 소형장치에 장착된 프로세서를 흔히 microprocessor라고 부른다.
데이터 포맷을 변환하는 역할을 수행하는 데이터 프로세싱 시스템(데이터 처리 시스템)을 의미한다.
데이터 포맷을 변환하는 역할을 수행하는 데이터 프로세싱 시스템(데이터 처리 시스템)을 의미한다.
출력 가능한 인쇄물을 생성하는 워드프로세서도 프로세서라고 한다.
출력 가능한 인쇄물을 생성하는 워드프로세서도 프로세서라고 한다.✔️ 프로세스 (Process)프로세스란 실제 메모리에 적재되어 프로세서에 의해 실행되고 있는 프로그램을 의미한다.
즉, 프로그램을 구동하였을 때, 프로그램 자체와 상태가 메모리 상에서 실행되는 작업의 단위를 지칭한다.✔️ 프로그램 (Program)프로그램은 일반적으로 보조기억장치(하드 디스크, SSD)에 저장되어 있는 실행코드(명령어)와 정적인 데이터를 의미한다. ✔️ 스레드 (Thread)스레드는 프로세스 내에서 실행되는 흐름의 단위를 말한다.
프로세스가 할당받은 자원을 이용하는 실행의 단위이다.❓ 프로세스 vs 스레드앞에서 의미를 설명한 것처럼 프로세스는 실행될 때 OS로부터 메모리, 필요한 주소 공간등의 자원을 할당받아 실행되는 프로그램을 의미하며, 스레드는 한 프로세스 내에서 동작되는 여러 실행의 흐름을 의미한다.
프로세스 내의 자원들은 같은 프로세스 상에 있는 스레드들끼리 공유하며 실행된다.[OS] POSIX Pthreads[OS] 멀티프로세싱, 멀티프로그래밍, 멀티태스킹, 멀티스레딩"
16,[Algorithm] 깊이우선탐색(DFS)과 너비우선탐색(BFS),https://velog.velcdn.com/images/falling_star3/post/ab4f220f-5c5b-4aeb-8b53-9a6473be8926/algorithm.png,"Algorithm📌 깊이우선탐색(DFS)과 너비우선탐색(BFS)▶ 해당 개념을 알기 위해서는 세 가지의 개념이 선행되어야 한다.1) 먼저 자료구조 스택/큐 이해가 필요하다.
스택/큐에 대한 설명은 아래 링크에 따로 정리했다.
👉🏻 https://velog.io/@falling_star3/자료구조-스택Stack큐Queue덱Deque2) 재귀함수의 호출과 리턴 과정에 대한 이해가 필요하다.
재귀함수에 대한 설명은 아래 링크에 따로 정리했다.
👉🏻 https://velog.io/@falling_star3/Python-재귀함수Recursive-Function3) 간선과 노드 및 인접행렬과 인접리스트에 대한 그래프 이해가 필요하다.
그래프에 대한 설명은 아래 링크에 따로 정리했다.
👉🏻 https://velog.io/@falling_star3/그래프Graph-인접행렬과-인접리스트4) BFS/DFS를 처음 접하는 사람은 아래 링크의 유튜브 설명을 보는 것을 추천한다.
BFS/DFS를 이해하기 정말 좋은 영상이고, 이 포스팅도 해당영상을 기반으로 했다.
👉🏻 https://www.youtube.com/watch?v=_hxFgg7TLZQ▶ 위의 그림은 DFS와 BFS 경로를 순서대로 나타낸 것이다.해당 그림을 통해 직관적으로 DFS와 BFS의 차이를 알 수 있을 것이다.
깊이우선탐색인 DFS는 가장 깊은 곳까지 방문하고, 너비우선탐색인 BFS는 같은 레벨 인접 노드를 전부 방문한뒤 다음 레벨 인접 노드를 방문한다.  DFS는 Stack을 통해 구현하고, BFS는 Queue를 통해 구현한다.✏️ 깊이우선탐색(DFS) 구현👉🏻 깊이우선탐색 DFS(Depth-First Search)는 그래프에서 깊은 부분을 우선적으로 탐색하는 알고리즘이다.탐색 시작 노드를 스택에 삽입하고 방문 처리를 한다.스택의 최상단 노드에 방문하지 않은 인접 노드가 있으면 그 인접 노드를 스택에 넣고 방문 처리를 한다. 방문하지 않은 인접 노드가 없으면 스택에서 최상단 노드를 꺼낸다.2번 과정을 더 이상 수행할 수 없을 때까지 반복한다.▶ 해당 그래프를 DFS로 구현해보자. DFS는 Stack을 이용한다. Stack은 한 방향에서만 자료를 넣고 뺄 수 있는 후입선출 방식 구조이므로, 가장 늦게 들어온 노드를 가장 먼저 뺄 수 있다. 또한, 한 번 담았던 노드는 다시 담지 않는다.▶ 먼저 시작 노드인 0번 노드를 스택에 담았다. ▶ 이 후 0번 노드를 꺼내 출력하고, 그 인접 노드인 1번 노드를 스택에 담았다.▶ 이 후 1번 노드를 꺼내 출력하고, 그 인접 노드인 2번 노드와 3번 노드를 스택에 담았다.▶ 이 후 3번 노드를 꺼내 출력하고, 그 인접 노드인 4번 노드와 5번 노드를 스택에 담았다.
    2번 노드는 이미 스택에 담겨있으므로 스택에 다시 추가하지 않는다.▶ 이 후 5번 노드를 꺼내 출력하고, 그 인접 노드인 6번 노드와 7번 노드를 스택에 담았다.▶ 이 후 7번 노드를 꺼내 출력하고, 인접 노드가 없으므로 더 담지 않는다.▶ 이 후 6번 노드를 꺼내 출력하고, 그 인접 노드인 8번 노드를 스택에 담았다.▶ 이 후 8번 노드를 꺼내 출력하고, 인접 노드가 없으므로 더 담지 않는다.▶ 이 후 4번 노드를 꺼내 출력하고, 2번 노드를 꺼내 출력한다. 더 꺼낼 노드가 없으므로 순회는 종료한다.DFS 경로 : 0 > 1 > 3 > 5 > 7 > 6 > 8 > 4 > 2💡 출력 순서와 과정은 스택이기 때문에 위와 같이 이루어지는 것이다. 스택은 마치 박스 쌓기와 같아서 가장 늦게 올린 박스를 가장 먼저 꺼낼 수 있다. DFS 구현 과정 또한 스택으로 구현하는 것이기에 가장 위에 있는 노드를 계속 꺼내는 것이다.✏️ 너비우선탐색(BFS)👉🏻 BFS(Breadth First Search)는 그래프에서 가까운 노드부터 탐색하는 알고리즘이다.탐색 시작 노드를 큐에 삽입하고 방문 처리를 한다.큐에서 노드를 꺼내 해당 노드의 인접 노드 중에서 방문하지 않은 노드를 모두 큐에 삽입하고 방문 처리를 한다.2번의 과정을 더 이상 수행할 수 없을 때까지 반복한다.▶ 해당 그래프를 BFS로 구현해보자. BFS는 Queue를 이용한다. Queue는 가장 먼저 들어온 것이 가장 먼저 나가는 선입선출 방식의 구조이다. DFS 구현과 마찬가지로 한번 큐에 담았던 노드는 다시 담지 않는다.▶ 먼저 시작 노드인 0번 노드를 큐에 담았다.▶ 이 후 0번 노드를 꺼내 출력하고, 그 인접 노드인 1번 노드를 큐에 담았다.
    큐는 선입선출이므로 꺼내는 방향이 그림의 화살표처럼 아래부터이다.▶ 이 후 1번 노드를 꺼내 출력하고, 그 인접 노드인 2번 노드와 3번 노드를 큐에 담았다.▶ 이 후 2번 노드를 꺼내 출력하고, 그 인접 노드인 4번 노드를 큐에 담았다.💡 큐는 선입선출 방식이므로 가장 아래 있는 2번 노드부터 꺼낸다. 또한, 그 인접 노드 중 1번, 3번은 이미 큐에 들어갔었으므로 4번 노드만 큐에 담긴다.▶ 이 후 3번 노드를 꺼내 출력하고, 그 인접 노드인 5번 노드를 큐에 담았다.▶ 이 후 4번 노드를 꺼내 출력하고, 그 인접 노드는 전부 큐에 담았던 적이 있으므로 다시 담지 않는다.▶ 이 후 5번 노드를 꺼내 출력하고, 그 인접 노드인 6번 노드와 7번 노드를 큐에 담았다.▶ 이 후 6번 노드를 꺼내 출력하고, 그 인접 노드인 8번 노드를 큐에 담았다.▶ 이 후 7번 노드를 꺼내 출력하고, 8번 노드를 꺼내 출력한다. 더 꺼낼 노드가 없으므로 순회는 종료한다.BFS 경로 : 0 > 1 > 2 > 3 > 4 > 5 > 6 > 7 > 8 ✏️ DFS와 BFS 예제백준에서 대표문제인 1260번 DFS와 BFS예제가 있다.
아래 링크에 해당 문제와 구현을 자세히 적어놨다.
https://velog.io/@falling_star3/백준Python-1260번-DFS와BFS▶ DFS 예제위와 같이 과정이 이루어진다.▶ BFS 예제위와 같이 과정이 이루어진다.참고<이것이 코딩테스트다>
https://www.youtube.com/watch?v=_hxFgg7TLZQ[Algorithm] 기본 정렬 알고리즘(Selection, Insertion,  Bubble, Merge, Quick, Heap)"
17,[알고리즘] 다익스트라(Dijkstra) 알고리즘,https://velog.velcdn.com/images/717lumos/post/01157c60-ae28-4fb0-892d-f1927abfc844/%ED%91%9C%EC%A7%801.png,"Data Structure & Algorithm1-1. 개요다익스트라(dijkstra) 알고리즘은 그래프에서 한 정점(노드)에서 다른 정점까지의 최단 경로를 구하는 알고리즘 중 하나이다. 이 과정에서 도착 정점 뿐만 아니라 모든 다른 정점까지 최단 경로로 방문하며 각 정점까지의 최단 경로를 모두 찾게 된다. 매번 최단 경로의 정점을 선택해 탐색을 반복하는 것이다.참고로 그래프 알고리즘 중 최소 비용을 구하는 데는 다익스트라 알고리즘 외에도 벨만-포드 알고리즘, 프로이드 워샬 알고리즘 등이 있다.1-2. 동작 단계① 출발 노드와 도착 노드를 설정한다.
② '최단 거리 테이블'을 초기화한다.
③ 현재 위치한 노드의 인접 노드 중 방문하지 않은 노드를 구별하고, 방문하지 않은 노드 중 거리가 가장 짧은 노드를 선택한다. 그 노드를 방문 처리한다.
④ 해당 노드를 거쳐 다른 노드로 넘어가는 간선 비용(가중치)을 계산해 '최단 거리 테이블'을 업데이트한다.
⑤ ③~④의 과정을 반복한다.'최단 거리 테이블'은 1차원 배열로, N개 노드까지 오는 데 필요한 최단 거리를 기록한다. N개(1부터 시작하는 노드 번호와 일치시키려면 N + 1개) 크기의 배열을 선언하고 큰 값을 넣어 초기화시킨다.'노드 방문 여부 체크 배열'은 방문한 노드인지 아닌지 기록하기 위한 배열로, 크기는 '최단 거리 테이블'과 같다. 기본적으로는 False로 초기화하여 방문하지 않았음을 명시한다.1-3. 동작 예출발 노드는 1번, 도착 노드는 6번이라 하고 거리 테이블을 전부 큰 값, 여기서는 inf(무한대)로 초기화했다. 각 노드들을 잇는 간선의 가중치 역시 표시했다.출발 노드를 먼저 선택하고 거리를 0으로 한다.1번 노드와 연결된 인접 노드는 2, 4이다. 그곳까지 가는 거리를 각각 기존의 거리값과 비교해 최솟값으로 업데이트한다. 가령 2의 경우 inf와 2 중 작은 값인 2를 택해 할당한다.
또한 인접 노드까지의 거리를 모두 업데이트한 1번 노드는 방문 표시를 한다.
최근 갱신한 테이블 기준, 방문하지 않는 노드 중 가장 거리값이 작은 번호를 다음 노드로 택한다. 위 상태에서는 4번 노드이다.4번 노드에서 같은 작업을 수행한다. 4번과 연결된 2, 5번까지 오는 거리를 계산한다. 가령 5의 경우엔 4번까지 오는 데 필요한 거리 + 4->5 간선의 가중치 값인 2와 기존의 값인 inf 중 최솟값을 계산하고, 2번 노드의 경우엔 기존 값인 2와 4번을 거쳐가는 값인 1+2 = 3을 비교한다. 그렇다면 2번 노드는 기존 값이 더 크므로 업데이트하지 않는다. 즉, 1번에서 바로 2번으로 가는 것이 1->4를 거쳐 2번으로 가는 길보다 더 적게 걸린단 뜻이다.
다음으로 선택될 노드는 아직 방문하지 않은 노드 2, 3, 5, 6 중 거리값이 가장 작은 노드이므로 2 또는 5이다. 거리 값이 같다면 일단 인덱스가 작은 노드를 택한다고 하고 2번으로 가보자.2번 노드와 연결된 3, 4번 노드에 대해 같은 과정을 반복한다.
그 다음 노드는 3, 5, 6번 중 가장 거리값이 작은 5번 노드가 되겠다.5번 노드와 연결된 6번 노드에 같은 과정을 반복한다.
다음 노드를 선택해야 하는데, 아직 방문하지 않은 3번과 6번 중 거리값이 작은 것은 6번이다. 그런데 6번은 더 이어지는 노드도 없는데다 도착 노드이다. 따라서 알고리즘을 종료한다.최종적으로는 1번에서 6번까지 오는 데 1 - 4 - 5 - 6의 경로를 거치고 최소 거리는 4가 된다.1-4. 특징위 동작 예시에서 볼 수 있듯이, 다익스트라 알고리즘은 방문하지 않은 노드 중 최단 거리인 노드를 선택하는 과정을 반복한다.
또한 각 단계마다 탐색 노드로 한 번 선택된 노드는 최단 거리를 갱신하고, 그 뒤에는 더 작은 값으로 다시 갱신되지 않는다.
도착 노드는 해당 노드를 거쳐 다른 노드로 가는 길을 찾을 필요는 없다.다익스트라 알고리즘은 가중치가 양수일 때만 사용 가능하다는 중요한 특징이 있다.위 그림의 상황에서 1 → 4의 경로가 최단경로이려면 3+k가 1보다 커야 한다. 즉, k > -2가 성립해야 한다. 반대로 k가 -5라고 한다면 오히려 1 → 2 → 4 경로가 더 최단 경로가 된다. 그 말인 즉, 1번에서 연결된 노드 중 4번이 가중치가 적다는 이유로 최단 거리를 1이라 할 수는 없다는 이야기이다.
따라서 다익스트라 알고리즘을 사용하기 위해서는 정점 사이를 잇는 간선의 가중치가 양수여야 한다. 그래야 한 번 방문한 정점에 대해서는 값을 업데이트 하지 않아도 되는 것이다.1-5. 구현 방법다익스트라 알고리즘을 구현하는 방법에는 '방문하지 않은 노드'를 다루는 방식에서 '순차 탐색'을 할 것이나 '우선순위 큐'를 쓸 것이냐로 나뉜다. 자세한 방식은 아래에서 계속한다.'방문하지 않은 노드 중 거리값이 가장 작은 노드'를 선택해 다음 탐색 노드로 삼는다. 그 노드를 찾는 방식이 순차 탐색이 된다. 즉 거리 테이블의 앞에서부터 찾아내야 하므로 노드의 개수만큼 순차 탐색을 수행해야 한다. 따라서 노드 개수가 N이라고 할 때 각 노드마다 최소 거리값을 갖는 노드를 선택해야 하는 순차 탐색이 수행되므로 (N−1)×N=O(N2)(N-1) \times N = O(N^2)(N−1)×N=O(N2)의 시간이 걸린다.아래 코드에서 dist[]는 각 노드까지의 최소 거리를 저저장하고, visited[]는 방문 여부를, map[][]은 한 노드에서 다른 노드로의 거리를 저장하고 있다.파이썬으로 구현한 코드는 아래 링크를 참고한다.
💎 [Python] 최단경로를 위한 다익스트라(Dijkstra) 알고리즘3-1. 특징순차 탐색을 사용할 경우 노드 개수에 따라 탐색 시간이 매우 오래 걸릴 수 있다. 이를 개선하기 위해 우선순위 큐를 도입하기도 한다.
거리 값을 담을 우선순위 큐는 힙으로 구현하고, 만약 최소 힙으로 구현한다면 매번 루트 노드가 최소 거리를 가지는 노드가 될 것이다.
파이썬의 경우 PriorityQueue나 heapq 라이브러리로 우선순위 큐, 최소 힙이 지원되며, C++의 경우 <queue> 라이브러리에서 최대 힙을 지원하는 priority_queue 를 사용할 수 있다. 최대 힙을 최소 힙으로 쓰려면 저장되는 값에 -를 붙여 음수로 만들면 된다.우선순위 큐에서 사용할 '우선순위'의 기준은 '시작 노드로부터 가장 가까운 노드'가 된다. 따라서 큐의 정렬은 최단 거리인 노드를 기준으로 최단 거리를 가지는 노드를 앞에 배치한다.위의 순차 탐색을 쓰는 구현과는 다르게 우선순위 큐를 사용하면 방문 여부를 기록할 배열은 없어도 된다. 우선순위 큐가 알아서 최단 거리의 노드를 앞으로 정렬하므로 기존 최단 거리보다 크다면 무시하면 그만이다. 만약 기존 최단거리보다 더 작은 값을 가지는 노드가 있다면 그 노드와 거리를 우선순위 큐에 넣는다. 우선순위 큐에 삽입되는 형태는 <거리, 노드> 꼴이다.3-2. 동작 예시출발 노드인 1번의 거리를 업데이트하고 우선순위 큐에 넣는다.큐에서 맨 앞 노드인 <거리 0, 노드 1>을 꺼내 그 인접 노드를 조사한다. 거리값이 업데이트 되는 노드만, 즉 최소 거리로 업데이트 되는 노드만 큐에 넣는다. 큐는 거리값이 작은 순서대로 정렬된다.<거리 1, 노드 4>를 꺼내 그 인접 노드를 조사한다. 최소 거리로 거리가 업데이트 된 5번을 큐에 넣는다. 2번의 경우 기존의 거리값인 2보다 1 + 2 = 3이 더 크므로 2번은 큐에 넣지 않는다.같은 과정의 반복으로 <거리 2, 노드 2>의 인접 노드를 조사하고 거리값이 갱신된 노드만 큐에 넣는다. 3번이 큐에 들어간다.큐가 빌 때까지 해당 과정을 반복한다.최종적으로 도착 노드의 거리 값이 최소 거리가 된다.3-3. 코드 구현그래프의 형태는 '시작 노드 / 끝 노드 / 그 사이 간선의 가중치'를 입력 받아 인접 행렬 graph[][]에 넣어 표현한다.파이썬으로 구현한 코드는 아래 링크를 참고한다.
💎 Python으로 다익스트라(dijkstra) 알고리즘 구현하기
💎 [Python] 우선순위 큐를 활용한 개선된 다익스트라(Dijkstra) 알고리즘3-4. 시간 복잡도간선의 수를 E(Edge), 노드의 수를 V(Vertex)라고 했을 때 O(E logV)O(E \ \mathrm logV)O(E logV)가 된다.
우선순위 큐에서 꺼낸 노드는 연결된 노드만 탐색하므로 최악의 경우라도 총 간선 수인 E만큼만 반복한다. 즉 하나의 간선에 대해서는 O(logE)O(\mathrm logE)O(logE)이고, E는 V2V^2V2보다 항상 작기 때문에 E개의 간선을 우선순위 큐에 넣었다 빼는 최악의 경우에 대해서는 O(E logV)O(E \ \mathrm logV)O(E logV)이다.YounghunJo, ""
[Python] 최단경로를 위한 다익스트라(Dijkstra) 알고리즘,"" Tistory - 앎의 공간해피니s, ""[알고리즘] 다익스트라(Dijkstra) 알고리즘이란? | c++ 다익스트라 구현,"" Tistory - code_labMin Jae Park, ""Python으로 다익스트라(dijkstra) 알고리즘 구현하기,"" justkodeYounghunJo, ""[Python] 우선순위 큐를 활용한 개선된 다익스트라(Dijkstra) 알고리즘,"" Tistory - 앏의 공간얍문, ""[ 다익스트라 알고리즘 ] 개념과 구현방법 (C++),"" Tistory - 얍문's Coding World공부하는 식빵맘, ""Chapter 4-5. 다익스트라 최단 경로 알고리즘,"" GitHub blog - 평생 공부 블로그 : Today I Learned[알고리즘] 너비 우선 탐색(BFS)덕분에 좋은 내용 잘 보고 갑니다
감사합니다.와 설명 너무 깔끔하네요. 글 작성해주셔서 감사합니다."
18,"[알고리즘] 그리디, 탐욕 알고리즘 (Greedy Algorithm) ",https://velog.velcdn.com/images/realhsb/post/b0236ad9-440e-44d2-b9cd-8de6bfc08451/image.png,"알고리즘Greedy : '탐욕스러운, 욕심 많은'이라는 뜻탐욕 알고리즘이란, 선택의 순간마다 지금 당장 좋은 상황만 쫓아 최종적인 해답에 도달하는 방식탐욕 알고리즘은 최적해를 구하는 데 사용되는 근사적인 방식탐욕 알고리즘은 여러 경우 중 하나를 결정해야 할 때마다 그 순간에 최적이라고 생각되는 것을 선택해 나가는 방식으로 진행하여 최종적인 해답에 도달순간마다 하는 선택은 그 순간에 대해 지역적으로는 최적이지만, 그 선택들을 통하여 만든 전역적인 해답이 최적이라는 보장은 없다.하지만 탐욕 알고리즘을 적용하는 코딩 테스트 문제들은 지역적으로 최적이면서 전역적으로 최적인 문제들이다. 즉, 탐욕법으로 구한 답이 최적해가 되게끔 만든다. 그렇기 때문에 우리는 이것을 추론할 줄 알아야 한다.📌 탐욕 알고리즘 문제 해결 방법선택 절차(Selection Procedure): 현재 상태에서의 최적의 해답을 선택한다.적절성 검사(Feadibility Check): 선택된 해가 문제의 조건을 만족하는지 검사한다.해답 검사(Solution Check): 원래의 문제가 해결되었는지 검사하고, 해결되지 않았다면 선택 절차로 돌아가 위의 과정을 반복한다.📌 탐욕 알고리즘을 적용하기 위한 2가지 조건탐욕 알고리즘이 잘 작동하는 문제는 대부분 탐욕스런 선택 조건(greedy choice property)과 최적 부분 구조 조건(optimal substructure)이라는 두 가지 조건이 만족된다.탐욕스런 선택 조건은 앞의 선택이 이후의 선택에 영향을 주지 않는다는 것이며, 최적 부분 구조 조건은 문제에 대한 최적해가 부분문제에 대해서도 역시 최적해라는 것이다.탐욕적 선택 속성(Greedy Choice Property) : 앞의 선택이 이후의 선택에 영향을 주지 않는다.최적 부분 구조(Optimal Substructure) : 문제에 대한 최종 해결 방법은 부분 문제에 해단 최적 문제 해결 방법으로 구성된다.이러한 조건이 성립하지 않는 경우에는 탐욕 알고리즘은 최적해를 구하지 못한다.하지만, 이러한 경우에도 탐욕 알고리즘은 근사 알고리즘으로 사용이 가능할 수 있으며, 대부분의 경우 계산 속도가 빠르기 때문에 실용적으로 사용할 수 있다.어떤 특별한 구조가 있는 문제에 대해서는 탐욕 알고리즘이 언제나 최적해를 찾아낼 수 있는 구조를 매트로이드라고 한다.탐욕 알고리즘은 항상 최적의 결과 도출하는 것은 아님. 그렇지만 어느 정도 최적에 근사한 값을 빠르게 도출 가능. 이 장점으로 이해 탐욕 알고리즘은 근사 알고리즘으로 사용할 수 있다.
이 그래프에서 노드 값의 합이 최대로 되는 최적해는 5 + 7 + 9 = 21이다.
탐욕 알고리즘을 통해 매 상황에서 큰 값만 고르면 5 + 10 + 4 = 19가 나온다. 👀 근사 알고리즘(Approximation Algorithm)이란?어떤 최적화 문제에 대한 해의 근사값을 구하는 알고리즘을 뜻한다.최적해를 구할 수 없지만, 비교적 빠른 시간에 계산이 가능하며, 어느 정도 보장된 근사해를 계산할 수 있다.📌 언제 탐욕 알고리즘을 사용할까?일반적인 상황에서 탐욕 알고리즘은 최적의 해를 보장하지 않을 때가 많다.하지만 코딩 테스트에서의 그리디 문제는 탐욕법으로 얻은 해가 최적의 해가 되는 상황에서, 이를 추론할 수 있어야 풀리도록 출제된다.🎁 탐욕 알고리즘 예제 - 거스름돈당신은 음식점의 계산을 도와주는 점원입니다. 카운터에는 거스름돈으로 사용할 500원, 100원, 50원, 10원짜리 동전이 무한히 존재한다고 가정합니다. 손님에게 거슬러 주어야 할 돈이 원일 때 거슬러 주어 야 할 동전의 최소 개수를 구하세요. 단, 거슬러 줘야 할 돈 N은 항상 10의 배수입니다.N = 1260일 때를 가정.가치가 높은 순서대로 동전을 선택한다.500원짜리 2개, 100원짜리 2개, 50원짜리 1개, 마지막으로 10원짜리 1개를 선택한다.예제에 탐욕 알고리즘 적용선택 절차거스름돈의 동전 개수를 줄이기 위해 현재 가장 가치가 높은 동전을 우선 선택한다.적절성 검사1번 과정을 통해 선택된 동전들의 합이 거슬러 줄 금액을 초과하는지 검사한다.초과하면 가장 마지막에 선택한 동전을 삭제하고, 1번으로 돌아가 한 단계 작은 동전을 선택한다.해답 검사선택된 동전들의 합이 거슬러 줄 금액과 일치하는지 검사한다.액수가 부족하면 1번 과정부터 다시 반복한다.가장 큰 화례 단위부터 돈을 거슬러 주는 것이 최적의 해를 보장하는 이유는 무엇일까?가지고 있는 동전 중에서 큰 단위가 항상 작은 단위의 배수이므로 작은 단위의 동전들을 종합해 다른 해가 나올 수 없기 때문이다.만약에 800원을 거슬러 주어야 하는데 화폐 단위가 500원, 400원, 100원이라면?
500원 1개, 100원 3개 총 4개를 거슬러주는 것보다는 400원 2개를 거슬러주는 것이 최적의 해다. 이는 500원이 400원의 배수가 아니므로 그리디 알고리즘으로 해결할 수 없다.탐욕 알고리즘(그리디 알고리즘)에서는 이처럼 문제 풀이를 위한 최소한의 아이디어를 떠올리고 이것이 정당한지 검토할 수 있어야 한다‼️그 외 탐욕 알고리즘을 적용할 수 있는 것들최소신장트리 (Minimum Spanning Tree)다익스트라 (Dijkstra’s algorithm for shortest paths from a single source)허프만 코드 (Huffman codes / data-compression codes)자료 출처https://www.youtube.com/watch?v=2zjoKjt97vQ&list=PLRx0vPvlEmdAghTr5mXQxGpHjWqSz0dgC&index=2https://hanamon.kr/알고리즘-탐욕알고리즘-greedy-algorithm/https://medium.com/ivymobility-developers/algorithm-a168afcd3611[알고리즘] 순열 (Permutation)[알고리즘] 깊이 우선 탐색 알고리즘 (DFS, Depth-First Search)좋은 글이네요. 공유해주셔서 감사합니다."
19,[자료구조] 트리 (Tree),https://velog.velcdn.com/images%2Fkimdukbae%2Fpost%2F4680ea5f-ce25-492c-9358-c780bb98611d%2Fimage.png,"Datastructure사진의 출처 : 링크
트리(Tree)는 그래프의 일종으로 정점과 간선을 이용하여 데이터의 배치 형태를 추상화한 자료구조이다.
트리(Tree)는 그래프의 일종으로 정점과 간선을 이용하여 데이터의 배치 형태를 추상화한 자료구조이다.
서로 다른 두 노드를 연결하는 길이 하나뿐인 그래프를 트리라고 부른다.
서로 다른 두 노드를 연결하는 길이 하나뿐인 그래프를 트리라고 부른다.
힙(Heap)을 구현하는 방법 중 하나가 트리이다.
힙(Heap)을 구현하는 방법 중 하나가 트리이다.트리(Tree)의 특징
트리 자료구조는 일반적으로 대상 정보의 각 항목들을 계층적으로 구조화할 때 사용하는 비선형 자료구조이다.
트리 자료구조는 일반적으로 대상 정보의 각 항목들을 계층적으로 구조화할 때 사용하는 비선형 자료구조이다.
트리의 구조는 '데이터 저장'의 의미보다는 '저장된 데이터를 더 효과적으로 탐색'하기 위해서 사용된다.
트리의 구조는 '데이터 저장'의 의미보다는 '저장된 데이터를 더 효과적으로 탐색'하기 위해서 사용된다.
리스트와 다르게 데이터가 단순히 나열되는 구조 X --> 트리는 부모(parent)와 자식(child)의 계층적인 관계로 표현된다.
리스트와 다르게 데이터가 단순히 나열되는 구조 X --> 트리는 부모(parent)와 자식(child)의 계층적인 관계로 표현된다.
트리는 사이클이 없다. 
트리는 사이클이 없다. 
트리에서 루트노드를 제외한 모든 노드는 단 하나의 부모노드를 가진다.
트리에서 루트노드를 제외한 모든 노드는 단 하나의 부모노드를 가진다.트리(Tree) 순회트리의 순회란 트리의 각 노드를 체계적인 방법으로 탐색하는 과정을 의미한다. 노드를 탐색하는 순서에 따라 전위 순회, 중위 순회, 후위 순회 3가지로 분류된다.1. 전위 순회 (Preorder)루트노드 --> 왼쪽 서브트리 --> 오른쪽 서브트리 의 순서로 순회하는 방식이다. 깊이 우선 순회라고도 불린다.2. 중위 순회 (Inorder)왼쪽 서브트리 --> 노드 --> 오른쪽 서브트리 의 순서로 순회하는 방식이다. 대칭 순회라고도 불린다.3. 후위 순회 (Postorder)왼쪽 서브트리 --> 오른쪽 서브트리 --> 노드 의 순서로 순회하는 방식이다. 
트리 자료구조는 여러 가지 유형이 있는데, 그중 가장 기본이 되는 트리는 이진 트리(Binary Tree) 구조이다.
트리 자료구조는 여러 가지 유형이 있는데, 그중 가장 기본이 되는 트리는 이진 트리(Binary Tree) 구조이다.
이진 트리는 2개 이하의 자식노드를 가진다. (자식노드가 없거나 1개의 자식노드만 가지는 것도 가능!)
이진 트리는 2개 이하의 자식노드를 가진다. (자식노드가 없거나 1개의 자식노드만 가지는 것도 가능!)
2개의 자식노드 중에서 왼쪽의 노드를 Left Node라고 하고, 오른쪽의 노드를 Right Node라고 한다.
2개의 자식노드 중에서 왼쪽의 노드를 Left Node라고 하고, 오른쪽의 노드를 Right Node라고 한다.이진 트리의 종류편향 이진 트리 (Skewed Binary Tree)편향 이진 트리는 하나의 차수로만 이루어져 있는 경우를 의미한다. 이러한 구조는 배열(리스트)와 같은 선형 구조이므로 'Leaf Node'(가장 아래쪽에 위치한 노드) 탐색 시 모두 데이터를 전부 탐색해야 한다는 단점이 있어 효율적이지 못하다. (이를 보완하기 위해 높이 균형 트리라는 것이 있다.)포화 이진 트리 (Full Binary Tree)포화 이진 트리는 'Leaf Node'를 제외한 모든 노드의 차수가 2개로 이루어져 있는 경우를 의미한다. 이 경우 해당 차수에 몇 개의 노드가 존재하는지 바로 알 수 있으므로 노드의 개수를 파악할 때 용이한 장점이 있다.완전 이진 트리 (Complete Binary Tree)포화 이진 트리와 같은 개념으로 트리를 생성하지만, 모든 노드가 왼쪽부터 차근차근 생성되는 이진 트리를 의미한다.
※ 힙(Heap)은 완전 이진 트리의 일종이다!이진 탐색 트리 (Binary Search Tree)이진 탐색 트리(Binary Search Tree)는 탐색을 위한 이진 트리 기반의 자료구조이다. 아래와 같은 특징을 갖는다.
left node에는 부모노드보다 작은 값이 저장된다.
left node에는 부모노드보다 작은 값이 저장된다.
right node에는 부모노드와 값이 같거나 큰 값이 저장된다.
right node에는 부모노드와 값이 같거나 큰 값이 저장된다.
모든 노드는 중복된 값을 가지지 않는다.
모든 노드는 중복된 값을 가지지 않는다.예제를 통해 이진 탐색 트리에 대해 알아보자!(Ex) [28, 21, 15, 14, 32, 25, 18, 11, 30, 19]를 이진 탐색 트리의 형태로 만들어보자.위와 같이 이진 탐색 트리를 만들었다. 왜 이진 탐색 트리 형태로 만들까? 바로 데이터를 효율적으로 검색(탐색)할 수 있기 때문이다!
원하는 값을 찾을 때까지 현재의 노드값보다 찾고자하는 값이 작으면 왼쪽으로 움직이고, 크면 오른쪽으로 움직인다. 이렇게 원하는 값을 더 빠르게 찾을 수 있게 된다.이진 탐색 트리(Binary Search Tree) 구현 코드 (Python)시간 복잡도는 O(logN)이다. (배열(리스트)는 검색 시간 복잡도는 O(N)이다.)
배열(리스트)보다 검색(탐색)에 훨씬 효율적이다. 시간 복잡도를 줄이는 데 굉장히 효율적이다.※ [정리] 트리는 계층적인 관계 표현에 쓰이기 때문에, OS의 FileSystem 구조나 대용량의 데이터를 계층적으로 저장할 때 많이 쓰이는 자료구조가 되겠습니다.[Python] collections 모듈의 Counter[자료구조] 트라이 (Trie)안녕하세요, 깔끔한 글 잘 읽었습니다. 혹시 실례가 되지 않는다면 개인 TIL 블로그 포스팅에 순회 이미지를 사용해도 괜찮을까요?감사합니다 잘보고갑니다~!"
20,[알고리즘] 벨만-포드 알고리즘 (Bellman-Ford Algorithm),https://velog.velcdn.com/images%2Fkimdukbae%2Fpost%2Fa80f3ef5-4540-45f2-9913-895b653755e3%2Fimage.png,"Algorithm
벨만-포드 알고리즘은 한 노드에서 다른 노드까지의 최단 거리를 구하는 알고리즘이다.
벨만-포드 알고리즘은 한 노드에서 다른 노드까지의 최단 거리를 구하는 알고리즘이다.
간선의 가중치가 음수일 때도 최단 거리를 구할 수 있다.
간선의 가중치가 음수일 때도 최단 거리를 구할 수 있다.우리가 알고있는 다익스트라 알고리즘도 최단 거리를 구하는 알고리즘인데, '벨만-포드는 또 뭘까?'라는 생각이 들 수 있다. 다익스트라와 벨만-포드의 차이점에 대해 알아보자.벨만-포드 vs 다익스트라위 그림을 보자. 우리는 '1번 노드에서 3번 노드로 가는 최단 거리'를 구한다고 가정하자. 우리의 육안으로 보면 '1번 -> 3번'으로 가는 경로는 2가지이다. 1번 -> 3번(cost:10)과 1번 -> 2번 -> 3번(cost:20-15=5) 2가지로, '1번 노드에서 3번 노드로 가는 최단 거리'는 5이다.이제 육안으로 보지않고 다익스트라 알고리즘을 사용하게 되면 매번 방문하지 않은 노드 중에서 최단 거리가 가장 짧은 노드를 선택하므로 1번 -> 3번(cost:10)의 경로를 선택하게 된다. 이처럼 음수 간선이 존재하면 최단 거리를 찾을 수 없는 상황이 발생한다.반면에 벨만-포드 알고리즘을 사용하게 되면 매번 모든 간선을 전부 확인하므로 1번 -> 2번 -> 3번(cost:20-15=5)의 경로를 선택하여, 최단 거리를 찾을 수 있게 된다.정리하자면,[다익스트라 알고리즘]
매번 방문하지 않은 노드 중에서 최단 거리가 가장 짧은 노드를 선택하여 한 단계씩 최단 거리를 구해나간다.
매번 방문하지 않은 노드 중에서 최단 거리가 가장 짧은 노드를 선택하여 한 단계씩 최단 거리를 구해나간다.
음수 간선이 없다면 최적의 해를 찾을 수 있다. (음수 간선이 있을 때는 최적의 해를 찾을 수 X)
음수 간선이 없다면 최적의 해를 찾을 수 있다. (음수 간선이 있을 때는 최적의 해를 찾을 수 X)
시간 복잡도가 빠르다. (OElogV) --> 개선된 다익스트라 알고리즘 (우선순위 큐 사용)
시간 복잡도가 빠르다. (OElogV) --> 개선된 다익스트라 알고리즘 (우선순위 큐 사용)[벨만-포드 알고리즘]
(정점 - 1)번의 매 단계마다 모든 간선을 전부 확인하면서 모든 노드간의 최단 거리를 구해나간다. (<-->다익스트라와 차이점은 매 반복마다 모든 간선을 확인한다는 것이다. 다익스트라는 방문하지 않은 노드 중에서 최단 거리가 가장 가까운 노드만을 방문한다.)

다익스트라 알고리즘에서의 최적의 해를 항상 포함하게 된다.

(정점 - 1)번의 매 단계마다 모든 간선을 전부 확인하면서 모든 노드간의 최단 거리를 구해나간다. (<-->다익스트라와 차이점은 매 반복마다 모든 간선을 확인한다는 것이다. 다익스트라는 방문하지 않은 노드 중에서 최단 거리가 가장 가까운 노드만을 방문한다.)다익스트라 알고리즘에서의 최적의 해를 항상 포함하게 된다.
음수 간선이 있어도 최적의 해를 찾을 수 있다. (음수 간선의 순환을 감지할 수 있기 때문이다.)
음수 간선이 있어도 최적의 해를 찾을 수 있다. (음수 간선의 순환을 감지할 수 있기 때문이다.)
시간 복잡도가 느리다. O(VE)
시간 복잡도가 느리다. O(VE)※ 모든 간선의 비용이 양수일  때는 다익스트라를, 음수 간선이 포함되어 있으면 벨만-포드를 사용하면 된다.벨만-포드 알고리즘 수행과정벨만-포드의 과정은 아래와 같다.
출발 노드를 설정한다.
출발 노드를 설정한다.
최단 거리 테이블을 초기화한다.
최단 거리 테이블을 초기화한다.
다음의 과정을 (V(=정점) - 1)번 반복한다.

모든 간선 E개를 하나씩 확인한다.
각 간선을 거쳐 다른 노드로 가는 비용을 계산하여 최단 거리 테이블을 갱신한다.

다음의 과정을 (V(=정점) - 1)번 반복한다.모든 간선 E개를 하나씩 확인한다.각 간선을 거쳐 다른 노드로 가는 비용을 계산하여 최단 거리 테이블을 갱신한다.만약 음수 간선 순환이 발생하는지 체크하고 싶다면 3번 과정을 한 번 더 수행한다.
--> 이때 최단 거리 테이블이 갱신된다면 음수 간선 순환이 존재하는 것이다.음수 간선 순환을 왜 확인하는지 알아보자.사진 출처 : 링크그림에서 '2번, 3번, 5번 노드들' 음수 간선의 순환이 포함되어 있다. 만약 '2번 -> 5번'으로 가는 비용을 계산하면 2번 -> 3번 -> 5번(cost:2+1-4= -1)로 -1이 된다.
그러나 '2번 -> 5번'으로 가는 경로는 순환(cycle)이 있기 때문에 비용을 -1로 단정지을 수 없으며, 순환을 계속 돌게되면 '2번 -> 5번'으로 가는 비용을 무한히 줄일 수 있게 된다. 이는 1번 노드를 제외한 모든 노드에서도 비용을 무한히 줄일 수 있기 때문에 최단 거리를 구할 수 없게되므로 우리는 꼭 음수 간선 순환을 확인해주어야 한다.V - 1까지 모든 단계를 진행한 후, 다음 단계인 V번째 단계일 때도 최단 거리 테이블이 갱신된다면 최단 거리를 무한히 줄일려는 시도이므로 음수 간선 순환이 존재한다는 사실을 알 수 있다. 따라서 V번째 단계에서 최단 거리 테이블이 갱신 여부로 음수 간선 순환을 확인할 수 있다. (V - 1까지 단계를 진행하면 모든 노드에 대한 최단 거리가 확정된다.)벨만-포드 알고리즘 코드 (Python)[BOJ 11657]의 정답이기도 하다.시간 복잡도는 O(VE)이다. 
V번 반복에 대해서 해당 정점과 연결되어 있는 모든 간선(E)을 탐색해주기 때문에 시간 복잡도는 O(V*E) = O(VE)가 된다.참고 : https://velog.io/@qweadzs/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EB%B2%A8%EB%A7%8C-%ED%8F%AC%EB%93%9C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98
https://stalker5217.github.io/algorithm/bellman-ford/
https://ssungkang.tistory.com/entry/Algorithm-%EB%B2%A8%EB%A7%8C%ED%8F%AC%EB%93%9CBellman-Ford-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98
https://ratsgo.github.io/data%20structure&algorithm/2017/11/27/bellmanford/
https://deep-learning-study.tistory.com/587[알고리즘] 이분 탐색 / 이진 탐색 (Binary Search)너무 잘 정리 되어있어서 링크 좀 퍼가겠습니다!다익스트라 알고리즘의 경우에도 음수간선순환이 없다면 최단 경로를 찾을수 있지 않나요?덕분에 좋은 내용 잘 보고 갑니다.
정말 감사합니다.혹시나 헷갈리실 분들 있어서 적자면,
해당 벨만포드 vs 다익스트라 비교하는 그래프에서는 다익스트라를 통해 1에서 3까지 가는 최단경로가 찾아집니다.
3에서 2로가는 비용이 5인 간선이 추가된다면, 다익스트라가 음수간선에서 통하지 않는 사례가 될것같네요"
21,동적 계획법(Dynamic Programming),https://velog.velcdn.com/images/polynomeer/post/9d39e8f5-4596-4cad-b74c-9d673b2bfa9e/travelling_salesman_problem1.png,"알고리즘동적 계획법(Dynamic Programming, DP)은 큰 문제를 작은 문제로 나누어서 푸는 방식의 알고리즘이다. 동적 계획법은 처음 주어진 문제를 더 작은 문제들로 나눈 뒤 각 조각의 답을 계산하고, 이 답들로부터 원래 문제에 대한 답을 계산해 낸다는 점에서 분할 정복(Divide & Conquer, D&C)과 비슷하다. 하지만 가장 큰 차이점은 동적 계획법에서는 쪼개진 작은 문제가 중복되지만, 분할 정복은 절대로 중복될수가 없다는 점이다.다시 말하면, 동적 계획법과 분할 정복의 차이는 문제를 나누는 방식이다. 동적 계획법에서는 어떤 부분 문제는 두 개 이상의 문제를 푸는데 사용될 수 있기 때문에, 이 문제의 답을 여러 번 계산하는 대신 한 번만 계산하고 그 결과를 재활용함으로써 속도의 향상을 시킬 수 있다. 이때 이미 계산한 값을 저장해 두는 메모리를 캐시(cache)라고 부르며, 두 번 이상 계산되는 부분 문제를 중복되는 부분 문제(overlapping subproblems)라고 부른다.1. 동적 계획법의 조건• 두 가지 속성을 만족해야 동적 계획법으로 문제를 풀 수 있다.Overlapping Subproblem : 겹치는 부분(작은) 문제Optimal Substructure : 최적 부분구조1.1. Overlapping Subproblem겹치는 부분 문제(overlapping subproblem) 는 어떤 문제가 여러개의 부분문제(subproblem)으로 쪼개질 수 있을때 사용하는 용어이다. 이때 '부분 문제'란, 항상 새로운 부분 문제를 생성해내기 보다는 계속해서 같은 부분 문제가 여러번 재사용되거나 재귀 알고리즘을 통해 해결되는 문제를 가리킨다.대표적인 예로 피보나치 수열이 있다.
0 1 1 2 3 5 8 13 21 34 55 89 ...피보나치 수열은 앞의 두 수를 더한 수가 다음 수가 되는 수열이다.F0 = 0F1 = 1Fn = Fn-1 + Fn-2 (n ≥ 2)겹치는 부분 문제가 있다면, 큰 문제는 작은 문제들을 통해 정답을 구할 수 있다. 큰 문제는 작은 문제와 같은 방법으로 풀 수 있으며, 모든 문제를 작은 문제로 쪼갤 수 있기 때문이다. (기저 사례를 제외한 모든 문제)• 문제: N번째 피보나치 수를 구하는 문제
• 작은 문제: N-1번째 피보나치 수를 구하는 문제, N-2번째 피보나치 수를 구하는 문제• 문제: N-1번째 피보나치 수를 구하는 문제
• 작은 문제: N-2번째 피보나치 수를 구하는 문제, N-3번째 피보나치 수를 구하는 문제• 문제: N-2번째 피보나치 수를 구하는 문제
• 작은 문제: N-3번째 피보나치 수를 구하는 문제, N-4번째 피보나치 수를 구하는 문제1.2. Optimal Substructure최적 부분구조(optimal substructure)는 어떤 문제의 최적의 해결책이 그 부분 문제의 최적의 해결책으로 부터 설계될 수 있는 경우를 말한다. 즉, 최적 부분구조 일때 문제의 정답을 작은 문제의 정답에서 부터 구할 수 있다. 이 속성은 동적 계획법이나 그리디 알고리즘의 유용성을 판별하는데 사용되기도 한다. (예시)
• 서울에서 부산을 가는 가장 빠른 길이 대전과 대구를 순서대로 거쳐야 한다면
• 대전에서 부산을 가는 가장 빠른 길은 대구를 거쳐야 한다.서울->대전->대구->부산 : 가장 빠른 경로Q) 대전->부산
A) 대전->대구->부산 : 가장 빠른 경로만약 가장 빠른 경로가 서울->대전->울산->부산 이라면Q) 대전->부산
A) 대전->울산->부산 : 가장 빠른 경로다시 피보나치 문제로 돌아와보면• 문제: N번째 피보나치 수를 구하는 문제
• 작은 문제: N-1번째 피보나치 수를 구하는 문제, N-2번째 피보나치 수를 구하는 문제
-> 문제의 정답을 작은 문제의 정답을 합하는 것으로 구할 수 있다.• 문제: N-1번째 피보나치 수를 구하는 문제
• 작은 문제: N-2번째 피보나치 수를 구하는 문제, N-3번째 피보나치 수를 구하는 문제
-> 문제의 정답을 작은 문제의 정답을 합하는 것으로 구할 수 있다.• 문제: N-2번째 피보나치 수를 구하는 문제
• 작은 문제: N-3번째 피보나치 수를 구하는 문제, N-4번째 피보나치 수를 구하는 문제
-> 문제의 정답을 작은 문제의 정답을 합하는 것으로 구할 수 있다.• Optimal Substructure를 만족한다면, 문제의 크기에 상관없이 어떤 한 문제의 정답은
일정하다.
• 10번째 피보나치 수를 구하면서 구한 4번째 피보나치 수
• 9번째 피보나치 수를 구하면서 구한 4번째 피보나치 수
• …
• 5번째 피보나치 수를 구하면서 구한 4번째 피보나치 수
• 4번째 피보나치 수를 구하면서 구한 4번째 피보나치 수
• 4번째 피보나치 수는 항상 같다. -> 겹치는 부분문제의 정답이 항상 같다!이처럼 같은 값을 매번 구하는 것은 매우 비효율적이다. 이때 이를 해결할 수 있는 방법이 바로 메모이제이션(Memoization)이다.2. 메모이제이션(Memoization)동적 계획법에서 각 문제는 한 번만 풀어야 한다. 중복되는 부분 문제를 여러번 풀지 않는다는 뜻이다. Optimal Substructure를 만족하기 때문에 같은 문제는 구할 때마다 정답이 같다. 따라서 정답을 한 번 구했으면 그 정답을 캐시에 메모해놓는다. 이렇게 메모하는 것을 코드의 구현에서는 배열에 저장하는 것으로 할 수 있다. 이를 메모이제이션(Memoization)이라고 한다.위의 코드는 피보나치 수를 구하는 함수이다. 점화식을 그대로 표현하여 재귀호출을 하고 있다. F(0)과 F(1)은 더 이상 쪼갤 수 없으므로, 그냥 n을 리턴한다. 이것을 재귀호출 그대로 실행한다면 시간복잡도는 함수의 깊이가 N일때 O(2^N)이다. 이 경우에는 이진트리의 탐색속도와 같다. 다음은 fibonacci(5)를 호출했을때의 그림이다.이때 f(3)과 f(2)는 겹치는 부분문제이고 정답이 같으므로, 미리 캐시에 저장해두고 활용할 수 있다. 그래서 메모이제이션하면 부분문제에서 또 파생되는 부분문제의 가지를 모두 생략할 수 있다. (중복되므로) 즉, 모든 문제를 한 번씩만 푼다. 때문에 시간복잡도는 (문제의 개수 x 문제 1개를 푸는시간) 이 되는데, 문제의 개수가 N -> O(N), 문제 1개를 푸는 시간은 +연산 하나(함수의 시간복잡도) -> O(1)이므로, 전체 시간복잡도는 O(N) 이다. 이를 구현한 코드는 다음과 같다.이런식을 구현하면 될텐데, 여기에서 이미 계산한 부분문제의 경우 그 값을 그대로 사용하는 코드를 추가해야 한다. memo배열에 값이 0이면 답을 구하지 않았다는 뜻이고, 0이 아니면 답을 구한적이 있다(이전의 호출에서 해당 값을 구함)는 뜻이므로, 이를 활용하면 된다.3. 동적 계획법 구현 방식동적 계획법의 구현 방식에는 두 가지 방법이 있다.Top-down : 큰 문제를 작은 문제로 쪼개면서 푼다. 재귀로 구현Bottom-up : 작은 문제부터 차례대로 푼다. 반복문으로 구현Top-down과 Botton-up의 시간복잡도 차이는 문제에 따라 다를 수 있으므로 정확히 알 수는 없다. Top-down은 재귀호출을 하기때문에 스택의 사용으로 시간이 더 걸릴 것이라고 생각할 수 있겠지만, 실제로 그 차이는 크지 않다. 다만, 파이썬의 경우 재귀 호출 시 스택 오버 플로우(stack overflow)가 발생할 수 있기때문에, Bottom-up으로 구현하는 것이 좋다. C++과 JAVA에서는 재귀로 구현하는 것이 크게 문제가 되지 않는다.어떠한 상황에서는 오히려 재귀로 구현하는 것이 간단하게 보이는 경우도 있다. 예를 들면, Fn = Fn-10 + Fn-20 이라는 문제가 있다고 하자. 재귀에서 이를 구하려면 F(20)을 호출하면 F(10)+F(0)으로 두번의 호출만에 정답이 구해진다. 하지만 반복문으로 구현하면 F(1),F(2),... 과 같이 반복횟수가 많아진다.하지만 Top-down으로만 해결가능하거나 Bottom-up으로만 해결가능한 문제는 극히 드문 경우이므로, 아무거나 선택해서 사용하면 된다.3.1. Top-down큰 문제를 작은 문제로 나눈다.작은 문제를 푼다.작은 문제를 풀었으니, 이제 큰 문제를 푼다.피보나치 문제로 예를 들면,fibonacci(n)라는 문제를 풀기위해서문제를 작은 문제로 나눈다.
• fibonacci(n-1)과 fibonacci(n-2)로 문제를 나눈다.작은 문제를 푼다.
• fibonacci(n-1)과 fibonacci(n-2)를 호출해 문제를 푼다.작은 문제를 풀었으니, 이제 문제를 푼다.
• fibonacci(n-1)의 값과 fibonacci(n-2)의 값을 더해 문제를 푼다.코드는 앞에서 설명한것과 같이 재귀호출로 구현할 수 있다.3.2. Bottom-up문제를 크기가 작은 문제부터 차례대로 푼다.문제의 크기를 조금씩 크게 만들면서 문제를 점점 푼다.작은 문제를 풀면서 왔기 때문에, 큰 문제는 항상 풀 수 있다.반복하다 보면 가장 큰 문제를 풀 수 있다.똑같이 피보나치 문제로 예를 들면, 문제를 크기가 작은 문제부터 차례대로 푼다.
• for (int i=2; i<=n; i++)문제의 크기를 조금씩 크게 만들면서 문제를 점점 푼다.
• for (int i=2; i<=n; i++)작은 문제를 풀면서 왔기 때문에, 큰 문제는 항상 풀 수 있다.
• d[i] = d[i-1] + d[i-2];반복하다 보면 가장 큰 문제를 풀 수 있다.
• d[n]을 구하게 된다.따라서 다음과 같이 반복문으로 코드를 작성할 수 있다.4. 동적 계획법을 통한 문제풀이먼저, 문제에서 구하려고 하는 답을 문장으로 나타낸다. (예: 피보나치 수를 구하는 문제 -> N번째 피보나치 수) 이제 그 문장에 나와있는 변수의 개수만큼 메모하는 캐시 배열을 만든다. Top-down인 경우에는 재귀 호출의 인자의 개수가 된다. 마지막으로, 문제를 작은 문제로 나누고, 수식(점화식)을 이용해서 문제를 표현해야 한다.재귀(Recursion)순열(Permutation)동적계획법은 bottom up 이어야 한다고 생각했는데 그런 게 아니었군요긱스포긱스 보고 이해안되서 찾다가 왔는데 정리 좋네요. 감사합니다!"
22,[책 요약] 제텔카스텐 - 도입,https://velog.velcdn.com/images/hyemin916/profile/593cf1c8-283b-4a9d-8fa0-e523b2ef7e21/image.jpeg,"그 누구도 글을 쓰지 않고는 생각할 수 없다. -니클라스 루만글쓰기가 힘든 이유는 글쓰기의 시작이 빈 화면이라는 믿음 때문이다. 글쓰기 과정은 빈 화면을 마주하는 것보다 훨씬 더 먼저 시작된다. 좋은 글쓰기는 좋은 메모법을 바탕으로 한다. 이미 글로 표현된 것을 또 다른 글로 바꾸는 것이 비교할 수 없을 정도로 훨씬 쉽다. 1. 여러분이 알아야 할 모든 것메모 상자(제텔카스텐) 기법은 단순하지만, 익숙한 옛날 방식에서 변화된 작업 습관이 필요하다.좋은 결과물을 만들어내는 데에 강인한 의지력보다 중요한 것은 의지력이 필요하지 않는, 저항력이 발생하지 않는 작업 환경을 만들어내는 것이다.2. 여러분이 해야 할 모든 것임시 메모: 순간적으로 떠오른 아이디어를 나중에 다시 상기시키기 위한 메모이다. 문헌 메모: 무언가를 읽으면서 메모로 남기고 싶은 것을 짧게, 선별적으로, 나만의 표현으로 남긴다. 서지정보 시스템에 보관한다.영구보관용 메모: 임시 메모, 문헌 메모를 살펴보며 본인의 관심사와 어떻게 유의미하게 관련되는지 생각해본다. 이 작업은 임시 메모 내용의 의미를 까먹기 전에 작업하는 것이 좋다(하루에 한번). 아이디어 하나 하나마다 정확히 하나의 메모지에, 완전한 문장으로 출처를 밝히며 정확하고 명확하게 메모한다.모아둔 메모를 이용해 상향식으로 아이디어를 발전시킨다.3. 여러분이 지녀야 할 모든 것본질적이고 중요한 작업은 '생각하기'이다. 본질에 집중하고 불필요한 복잡함을 만들지 않도록 한다.정신이 분산되지 않은 생각할 수 있는 뇌, 믿을만한 메모 모음. 이것이 우리가 필요한 전부다.4. 명심해야 할 한두 가지""누구든 플루트를 어떻게 다루는지는 안다(한쪽 끝으로 바람을 불어넣으며서 연주할 음표에 따라 손가락으로 구멍을 누르면 된다). 하지만 그 누구도 시험 삼아 한 번 불었을 때 들리는 소리로 이 악기를 판단하지는 않는다."" - 65p어떤 도구를 사용하든 작업 방식에 대해 고민하고 적용해야 한다. 왜 메모 상자가 효과가 있는지를 이해하고 사용해야 한다.전개연산자는 왜 샌드위치를 망쳤을까?어설픈 지식의 저주"
